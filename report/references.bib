@article{smote,
  title        = {{SMOTE:} Synthetic Minority Over-sampling Technique},
  author       = {Kevin W. Bowyer and Nitesh V. Chawla and Lawrence O. Hall and W. Philip Kegelmeyer},
  year         = 2011,
  journal      = {CoRR},
  volume       = {abs/1106.1813},
  url          = {http://arxiv.org/abs/1106.1813},
  eprinttype   = {arXiv},
  eprint       = {1106.1813},
  timestamp    = {Mon, 13 Aug 2018 16:46:59 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1106-1813.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{imbalancelearn,
  title        = {Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning},
  author       = {Guillaume  Lema{{\^i}}tre and Fernando Nogueira and Christos K. Aridas},
  year         = 2017,
  journal      = {Journal of Machine Learning Research},
  volume       = 18,
  number       = 17,
  pages        = {1--5},
  url          = {http://jmlr.org/papers/v18/16-365}
}
@misc{cleanvision,
  title        = {Cleanlab/cleanvision: Automatically find issues in image datasets and practice data-centric computer vision.},
  author       = {Cleanlab},
  journal      = {GitHub},
  url          = {https://github.com/cleanlab/cleanvision}
}
@article{nipype,
  title        = {Nipype: A Flexible, Lightweight and Extensible Neuroimaging Data Processing Framework in Python},
  author       = {Gorgolewski, Krzysztof  and Burns, Christopher D. and Madison, Cindee  and Clark, Dav  and Halchenko, Yaroslav O. and Waskom, Michael L. and Ghosh, Satrajit S.},
  year         = 2011,
  journal      = {Frontiers in Neuroinformatics},
  volume       = {Volume 5 - 2011},
  doi          = {10.3389/fninf.2011.00013},
  issn         = {1662-5196},
  url          = {https://www.frontiersin.org/journals/neuroinformatics/articles/10.3389/fninf.2011.00013},
  abstract     = {<p>Current neuroimaging software offer users an incredible opportunity to analyze their data in different ways, with different underlying assumptions. Several sophisticated software packages (e.g., AFNI, BrainVoyager, FSL, FreeSurfer, Nipy, R, SPM) are used to process and analyze large and often diverse (highly multi-dimensional) data. However, this heterogeneous collection of specialized applications creates several issues that hinder replicable, efficient, and optimal use of neuroimaging analysis approaches: (1) No uniform access to neuroimaging analysis software and usage information; (2) No framework for comparative algorithm development and dissemination; (3) Personnel turnover in laboratories often limits methodological continuity and training new personnel takes time; (4) Neuroimaging software packages do not address computational efficiency; and (5) Methods sections in journal articles are inadequate for reproducing results. To address these issues, we present Nipype (Neuroimaging in Python: Pipelines and Interfaces; <uri xlink:href="http://nipy.org/nipype" xmlns:xlink="http://www.w3.org/1999/xlink">http://nipy.org/nipype</uri>), an open-source, community-developed, software package, and scriptable library. Nipype solves the issues by providing Interfaces to existing neuroimaging software with uniform usage semantics and by facilitating interaction between these packages using Workflows. Nipype provides an environment that encourages interactive exploration of algorithms, eases the design of Workflows within and between packages, allows rapid comparative development of algorithms and reduces the learning curve necessary to use different packages. Nipype supports both local and remote execution on multi-core machines and clusters, without additional scripting. Nipype is Berkeley Software Distribution licensed, allowing anyone unrestricted usage. An open, community-driven development philosophy allows the software to quickly adapt and address the varied needs of the evolving neuroimaging community, especially in the context of increasing demand for reproducible research.</p>}
}
⏎
@phdthesis{smote-adasyn-review,
  title        = {A Comparative Review of SMOTE and ADASYN in Imbalanced Data Classification},
  author       = {Brandt, Jakob and Lanzén, Emil},
  year         = 2021,
  url          = {https://urn.kb.se/resolve?urn=urn:nbn:se:uu:diva-432162},
  abstractnote = {In this thesis, the performance of two over-sampling techniques, SMOTE and ADASYN, is compared. The comparison is done on three imbalanced data sets using three different classification models and evaluation metrics, while varying the way the data is pre-processed. The results show that both SMOTE and ADASYN improve the performance of the classifiers in most cases. It is also found that SVM in conjunction with SMOTE performs better than with ADASYN as the degree of class imbalance increases. Furthermore, both SMOTE and ADASYN increase the relative performance of the Random forest as the degree of class imbalance grows. However, no pre-processing method consistently outperforms the other in its contribution to better performance as the degree of class imbalance varies.}
}
@inproceedings{smote-bsmote-adasyn-study,
  title        = {A Comparative Study of SMOTE, Borderline-SMOTE, and ADASYN Oversampling Techniques using Different Classifiers},
  author       = {Dey, Ishani and Pratap, Vibha},
  year         = 2023,
  booktitle    = {2023 3rd International Conference on Smart Data Intelligence (ICSMDI)},
  volume       = {},
  number       = {},
  pages        = {294--302},
  doi          = {10.1109/ICSMDI57622.2023.00060},
  keywords     = {Support vector machines;Credit cards;Fraud;Decision trees;Random forests;Load modeling;Cancer;Imbalanced datasets;Oversampling;Under-sampling;SMOTE;Borderline-SMOTE;ADASYN;Machine learning}
}
@article{image-processing,
  title        = {A survey of image processing techniques for plant extraction and segmentation in the field},
  author       = {Esmael Hamuda and Martin Glavin and Edward Jones},
  year         = 2016,
  journal      = {Computers and Electronics in Agriculture},
  volume       = 125,
  pages        = {184--199},
  doi          = {https://doi.org/10.1016/j.compag.2016.04.024},
  issn         = {0168-1699},
  url          = {https://www.sciencedirect.com/science/article/pii/S0168169916301557},
  keywords     = {Colour index-based segmentation, Threshold-based segmentation, Learning-based segmentation, Segmentation quality, Plant pixels, Plant extraction},
  abstract     = {In this review, we present a comprehensive and critical survey on image-based plant segmentation techniques. In this context, “segmentation” refers to the process of classifying an image into plant and non-plant pixels. Good performance in this process is crucial for further analysis of the plant such as plant classification (i.e. identifying the plant as either crop or weed), and effective action based on this analysis, e.g. precision application of herbicides in smart agriculture applications. The survey briefly discusses pre-processing of images, before focusing on segmentation. The segmentation stage involves the segmentation of plant against the background (identifying plant from a background of soil and other residues). Three primary plant extraction algorithms, namely, (i) colour index-based segmentation, (ii) threshold-based segmentation, (iii) learning-based segmentation are discussed. Based on its prevalence in the literature, this review focuses in particular on colour index-based approaches. Therefore, a detailed discussion of the segmentation performance of colour index-based approaches is presented, based on studies from the literature conducted in the recent past, particularly from 2008 to 2015. Finally, we identify the challenges and some opportunities for future developments in this space.}
}
@inproceedings{white-balance,
  title        = {When Color Constancy Goes Wrong: Correcting Improperly White-Balanced Images},
  author       = {Afifi, Mahmoud and Price, Brian and Cohen, Scott and Brown, Michael S.},
  year         = 2019,
  month        = {June},
  booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@article{plant-histogram,
  title        = {Plant Classification Method Using Histogram and Machine Learning for Smart Agriculture Applications},
  author       = {Yaman, Orhan and Tuncer, Türker},
  year         = 2024,
  journal      = {Acta Infologica},
  publisher    = {Istanbul University},
  volume       = 7,
  number       = 1,
  pages        = {17–28},
  doi          = {10.26650/acin.1070261},
  keywords     = {Plant classification, Smart agriculture, Histogram, Feature extraction, Machine learning},
  abstract     = {Due to its high potential and value, the Internet of things (IoT) has been used in various areas such as information security, industry 4.0, and smart agriculture. IoT is used in agriculture through the use of sensors, unmanned aerial vehicles (UAV), satellite technologies, robots, image processing, and artificial intelligence technologies. These smart agricultural practices increase production and quality and lead to savings in irrigation, thereby reducing environmental pollution during production. This study proposes an ultra-lightweight automated plant species classification method for smart agriculture applications. A UAV is used to acquire a new image dataset. An ultra-lightweight classification method is then used to classify the acquired plant species images. Our proposed ultra-lightweight computer vision model presents a histogram-based simple feature extraction function. The presented feature extractor uses histogram extraction and median filter in conjunction. The generated features are fed to two shallow classifiers, which are the support vector machine (SVM), and k nearest neighbor (KNN). The utilized SVM and KNN classifiers have attained 96.45% and 94.11% accuracies consecutively. The results demonstrate that this model is very capable of plant image classification and is ready for use in a physical agriculture environment.}
}
@inproceedings{adasyn,
  title        = {ADASYN: Adaptive synthetic sampling approach for imbalanced learning},
  author       = {Haibo He and Yang Bai and Garcia, Edwardo A. and Shutao Li},
  year         = 2008,
  booktitle    = {2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)},
  volume       = {},
  number       = {},
  pages        = {1322--1328},
  doi          = {10.1109/IJCNN.2008.4633969},
  keywords     = {Classification algorithms;Decision trees;Algorithm design and analysis;Training data;Machine learning;Accuracy;Machine learning algorithms}
}
@article{Ronneberger2015-yv,
  title        = {{U-Net}: Convolutional Networks for Biomedical Image Segmentation},
  author       = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year         = 2015,
  abstract     = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  primaryclass = {cs.CV},
  eprint       = {1505.04597}
}
@article{Tan2019-lw,
  title        = {{EfficientNet}: Rethinking model scaling for convolutional Neural Networks},
  author       = {Tan, Mingxing and Le, Quoc V},
  year         = 2019,
  journal      = {arXiv [cs.LG]},
  publisher    = {arXiv},
  abstract     = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3\% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
  primaryclass = {cs.LG}
}
@article{Kingma2014-ij,
  title        = {Adam: A method for stochastic optimization},
  author       = {Kingma, Diederik P and Ba, Jimmy},
  year         = 2014,
  abstract     = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  primaryclass = {cs.LG},
  eprint       = {1412.6980}
}
@book{Goodfellow-et-al-2016,
  title        = {Deep Learning},
  author       = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  year         = 2016,
  publisher    = {MIT Press},
  note         = {\url{http://www.deeplearningbook.org}}
}
@article{Shorten2019-rq,
  title        = {A survey on image data augmentation for deep learning},
  author       = {Shorten, Connor and Khoshgoftaar, Taghi M},
  year         = 2019,
  month        = dec,
  journal      = {J. Big Data},
  publisher    = {Springer Science and Business Media LLC},
  volume       = 6,
  number       = 1,
  copyright    = {https://creativecommons.org/licenses/by/4.0},
  abstract     = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
  language     = {en}
}
@incollection{Prechelt1998-mf,
  title        = {Early stopping - but when?},
  author       = {Prechelt, Lutz},
  year         = 1998,
  booktitle    = {Lecture Notes in Computer Science},
  publisher    = {Springer Berlin Heidelberg},
  address      = {Berlin, Heidelberg},
  series       = {Lecture notes in computer science},
  pages        = {55--69}
}
@article{Shoaib2023-dy,
  title        = {An advanced deep learning models-based plant disease detection: A review of recent research},
  author       = {Shoaib, Muhammad and Shah, Babar and Ei-Sappagh, Shaker and Ali, Akhtar and Ullah, Asad and Alenezi, Fayadh and Gechev, Tsanko and Hussain, Tariq and Ali, Farman},
  year         = 2023,
  month        = mar,
  journal      = {Front. Plant Sci.},
  publisher    = {Frontiers Media SA},
  volume       = 14,
  pages        = 1158933,
  copyright    = {https://creativecommons.org/licenses/by/4.0/},
  abstract     = {Plants play a crucial role in supplying food globally. Various environmental factors lead to plant diseases which results in significant production losses. However, manual detection of plant diseases is a time-consuming and error-prone process. It can be an unreliable method of identifying and preventing the spread of plant diseases. Adopting advanced technologies such as Machine Learning (ML) and Deep Learning (DL) can help to overcome these challenges by enabling early identification of plant diseases. In this paper, the recent advancements in the use of ML and DL techniques for the identification of plant diseases are explored. The research focuses on publications between 2015 and 2022, and the experiments discussed in this study demonstrate the effectiveness of using these techniques in improving the accuracy and efficiency of plant disease detection. This study also addresses the challenges and limitations associated with using ML and DL for plant disease identification, such as issues with data availability, imaging quality, and the differentiation between healthy and diseased plants. The research provides valuable insights for plant disease detection researchers, practitioners, and industry professionals by offering solutions to these challenges and limitations, providing a comprehensive understanding of the current state of research in this field, highlighting the benefits and limitations of these methods, and proposing potential solutions to overcome the challenges of their implementation.},
  keywords     = {convolutional neural networks; deep learning; image processing; machine learning; performance evaluation; plant disease detection; practical applications},
  language     = {en}
}
@unknown{unknown,
  title        = {Classification of iPSC-Derived Cultures Using Convolutional Neural Networks to Identify Single Differentiated Neurons for Isolation or Measurement},
  author       = {Patel, Purva and Ali, Lina and Kaushik, Uma and Wright, Mallory and Green, Kaylee and Waligorski, Jason and Kremitzki, Colin and Bachman, Graham and Elia, Serena and Buchser, William},
  year         = 2023,
  month        = 12,
  pages        = {},
  doi          = {10.1101/2023.12.24.573194}
}
@article{Tharwat2021-bn,
  title        = {Classification assessment methods},
  author       = {Tharwat, Alaa},
  year         = 2021,
  month        = jan,
  journal      = {Appl. Comput. Inform.},
  publisher    = {Emerald},
  volume       = 17,
  number       = 1,
  pages        = {168--192},
  copyright    = {http://creativecommons.org/licenses/by-nc-nd/4.0/},
  abstract     = {Classification techniques have been applied to many applications in various fields of sciences. There are several ways of evaluating classification algorithms. The analysis of such metrics and its significance must be interpreted correctly for evaluating different learning algorithms. Most of these measures are scalar metrics and some of them are graphical methods. This paper introduces a detailed overview of the classification assessment measures with the aim of providing the basics of these measures and to show how it works to serve as a comprehensive source for researchers who are interested in this field. This overview starts by highlighting the definition of the confusion matrix in binary and multi-class classification problems. Many classification measures are also explained in details, and the influence of balanced and imbalanced data on each metric is presented. An illustrative example is introduced to show (1) how to calculate these measures in binary and multi-class classification problems, and (2) the robustness of some measures against balanced and imbalanced data. Moreover, some graphical measures such as Receiver operating characteristics (ROC), Precision-Recall, and Detection error trade-off (DET) curves are presented with details. Additionally, in a step-by-step approach, different numerical examples are demonstrated to explain the preprocessing steps of plotting ROC, PR, and DET curves.},
  language     = {en}
}
@misc{keras_vgg16,
  title        = {VGG16 - Keras Applications},
  author       = {Keras Team},
  year         = 2024,
  note         = {Accessed: 2025-05-15},
  howpublished = {\url{https://keras.io/api/applications/vgg/}}
}
@misc{keras_mobilenetv2,
  title        = {MobileNetV2 - Keras Applications},
  author       = {Keras Team},
  year         = 2024,
  note         = {Accessed: 2025-05-15},
  howpublished = {\url{https://keras.io/api/applications/mobilenet/}}
}
@article{Gopalapillai2021-on,
  title        = {Convolution-based encoding of depth images for transfer learning in {RGB-D} scene classification},
  author       = {Gopalapillai, Radhakrishnan and Gupta, Deepa and Zakariah, Mohammed and Alotaibi, Yousef Ajami},
  year         = 2021,
  month        = nov,
  journal      = {Sensors (Basel)},
  publisher    = {MDPI AG},
  volume       = 21,
  number       = 23,
  pages        = 7950,
  copyright    = {https://creativecommons.org/licenses/by/4.0/},
  abstract     = {Classification of indoor environments is a challenging problem. The availability of low-cost depth sensors has opened up a new research area of using depth information in addition to color image (RGB) data for scene understanding. Transfer learning of deep convolutional networks with pairs of RGB and depth (RGB-D) images has to deal with integrating these two modalities. Single-channel depth images are often converted to three-channel images by extracting horizontal disparity, height above ground, and the angle of the pixel's local surface normal (HHA) to apply transfer learning using networks trained on the Places365 dataset. The high computational cost of HHA encoding can be a major disadvantage for the real-time prediction of scenes, although this may be less important during the training phase. We propose a new, computationally efficient encoding method that can be integrated with any convolutional neural network. We show that our encoding approach performs equally well or better in a multimodal transfer learning setup for scene classification. Our encoding is implemented in a customized and pretrained VGG16 Net. We address the class imbalance problem seen in the image dataset using a method based on the synthetic minority oversampling technique (SMOTE) at the feature level. With appropriate image augmentation and fine-tuning, our network achieves scene classification accuracy comparable to that of other state-of-the-art architectures.},
  keywords     = {RGB-D images; depth encoding; multimodal learning; scene classification; transfer learning},
  language     = {en}
}
@article{li2022improved,
  title        = {Improved AlexNet with Inception-V4 for Plant Disease Diagnosis},
  author       = {Li, Zhuoxin and Li, Cong and Deng, Linfan and Fan, Yanzhou and Xiao, Xianyin and Ma, Huiying and Qin, Juan and Zhu, Liangliang},
  year         = 2022,
  journal      = {Computational intelligence and neuroscience},
  publisher    = {Wiley Online Library},
  volume       = 2022,
  number       = 1,
  pages        = 5862600
}
@inproceedings{resnet18,
  title        = {Deep Residual Learning for Image Recognition},
  author       = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year         = 2016,
  booktitle    = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  volume       = {},
  number       = {},
  pages        = {770--778},
  doi          = {10.1109/CVPR.2016.90},
  keywords     = {Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation}
}
@inproceedings{hu2018squeeze,
  title        = {Squeeze-and-excitation networks},
  author       = {Hu, Jie and Shen, Li and Sun, Gang},
  year         = 2018,
  booktitle    = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages        = {7132--7141}
}
@inproceedings{rohman2024classification,
  title        = {Classification of Mango Plant Leaf Diseases Using Optimized ConvNeXt},
  author       = {Rohman, Ahmad Taufiq Nur and Sthevanie, Febryanti and Ramadhani, Kurniawan Nur},
  year         = 2024,
  booktitle    = {2024 International Conference on Intelligent Cybernetics Technology \& Applications (ICICyTA)},
  pages        = {443--448},
  organization = {IEEE}
}
@inproceedings{mobilenet,
  title        = {MobileNetV3 for Image Classification},
  author       = {Qian, Siying and Ning, Chenran and Hu, Yuepeng},
  year         = 2021,
  booktitle    = {2021 IEEE 2nd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)},
  volume       = {},
  number       = {},
  pages        = {490--497},
  doi          = {10.1109/ICBAIE52039.2021.9389905},
  keywords     = {Performance evaluation;Convolution;Neural networks;Feature extraction;Mobile handsets;Task analysis;Image classification;Convolution neural network;Image classification;Mobile devices;MobileNetV3}
}
@article{buslaev2020albumentations,
  title        = {Albumentations: fast and flexible image augmentations},
  author       = {Buslaev, Alexander and Iglovikov, Vladimir I and Khvedchenya, Eugene and Parinov, Alex and Druzhinin, Mikhail and Kalinin, Alexandr A},
  year         = 2020,
  journal      = {Information},
  publisher    = {Multidisciplinary Digital Publishing Institute},
  volume       = 11,
  number       = 2,
  pages        = 125
}
@article{selvaraju2016grad,
  title        = {Grad-CAM: Why did you say that?},
  author       = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
  year         = 2016,
  journal      = {arXiv preprint arXiv:1611.07450}
}
@article{Adhinata2021,
  title        = {Comparative Study of VGG16 and MobileNetV2 for Masked Face Recognition},
  author       = {Adhinata,  Faisal Dharma and Tanjung,  Nia Annisa Ferani and Widayat,  Widi and Pasfica,  Gracia Rizka and Satura,  Fadlan Raka},
  year         = 2021,
  month        = jul,
  journal      = {Jurnal Ilmiah Teknik Elektro Komputer dan Informatika},
  publisher    = {Universitas Ahmad Dahlan},
  volume       = 7,
  number       = 2,
  pages        = 230,
  doi          = {10.26555/jiteki.v7i2.20758},
  issn         = {2338-3070},
  url          = {http://dx.doi.org/10.26555/jiteki.v7i2.20758}
}

