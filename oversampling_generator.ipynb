{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2dc8e7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "config = {\n",
    "    \"csv_path\": \"processed_data/cleaned_imbalance_metadata.csv\",\n",
    "    \"label_encoder_path\": \"processed_data/le_cleaned_imbalance_metadata.npy\",\n",
    "    \"val_set_csv\": \"processed_data/new_val_metadata.csv\",\n",
    "    \"balanced_train_csv\": \"processed_data/new_balanced_train_metadata.csv\",\n",
    "    \"original_images_dir\": \"Dataset/train_images\",\n",
    "    \"augmented_images_dir\": \"Dataset/SMOT_images_temp\",\n",
    "    \"merged_output_dir\": \"Dataset/train_images_balanced\",\n",
    "\n",
    "}\n",
    "\n",
    "def load_and_preprocess_data(random_state=42):\n",
    "    df = pd.read_csv(config[\"csv_path\"])\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    df['label_encoded'] = le.fit_transform(df['label'])\n",
    "    print(f\"Label classes: {le.classes_}\")\n",
    "    \n",
    "    with open(config['label_encoder_path'], 'wb') as f:\n",
    "        np.save(f, le.classes_)\n",
    "    \n",
    "    train_df, val_df = train_test_split(\n",
    "        df, \n",
    "        test_size=0.2, \n",
    "        stratify=df['label'],\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    \n",
    "    train_df.to_csv(config['train_set_csv'], index=False)\n",
    "    val_df.to_csv(config['val_set_csv'], index=False)\n",
    "    \n",
    "    return train_df, val_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72f4819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_images_based_on_csv(csv_path, src_dir, dest_dir):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        src = os.path.join(src_dir, row['label'], row['image_id'])\n",
    "        dst = os.path.join(dest_dir, row['label'], row['image_id'])\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        shutil.copy2(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c368c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_augmented_images(df_resampled, original_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    aug = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.CLAHE(p=0.5),\n",
    "        A.HueSaturationValue(p=0.5, hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20),\n",
    "        A.RandomBrightnessContrast(p=0.5, brightness_limit=0.2, contrast_limit=0.2),\n",
    "        A.GaussNoise(p=0.5, var_limit=(10.0, 50.0)),        \n",
    "    ])\n",
    "\n",
    "    # Track existing augmented images to avoid duplicates\n",
    "    existing_augmented = set()\n",
    "    for label in os.listdir(output_dir):\n",
    "        label_dir = os.path.join(output_dir, label)\n",
    "        if os.path.isdir(label_dir):\n",
    "            existing_augmented.update(\n",
    "                os.path.join(label, f) \n",
    "                for f in os.listdir(label_dir) \n",
    "                if f.endswith('.jpg')\n",
    "            )\n",
    "\n",
    "    generated_count = 0\n",
    "\n",
    "    for _, row in tqdm(df_resampled.iterrows(), total=len(df_resampled)):\n",
    "        base_name = os.path.splitext(row['image_id'])[0]\n",
    "        new_filename = f\"{base_name}_aug_{random.randint(1,100000)}.jpg\"\n",
    "        output_path = os.path.join(output_dir, row['label'], new_filename)        \n",
    "\n",
    "            \n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        # Load original image\n",
    "        original_path = os.path.join(original_dir, row['label'], row['image_id'])\n",
    "        original_img = cv2.imread(original_path)\n",
    "        if original_img is None:\n",
    "            continue\n",
    "            \n",
    "        # Generate and save augmented version\n",
    "        augmented = aug(image=original_img)['image']\n",
    "        cv2.imwrite(output_path, augmented)\n",
    "        generated_count += 1\n",
    "\n",
    "    print(f\"Generated {generated_count} synthetic images in {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cfeca7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(original_dir, smote_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Merges original and SMOTE-augmented images into a single dataset.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Iterate through all class subdirectories\n",
    "    for label in tqdm(os.listdir(original_dir)):\n",
    "        original_label_dir = os.path.join(original_dir, label)\n",
    "        smote_label_dir = os.path.join(smote_dir, label)\n",
    "        output_label_dir = os.path.join(output_dir, label)\n",
    "        \n",
    "        os.makedirs(output_label_dir, exist_ok=True)\n",
    "        \n",
    "        for img_file in os.listdir(original_label_dir):\n",
    "            src = os.path.join(original_label_dir, img_file)\n",
    "            dst = os.path.join(output_label_dir, img_file)\n",
    "            if not os.path.exists(dst):\n",
    "                shutil.copy2(src, dst)\n",
    "        \n",
    "        if os.path.exists(smote_label_dir):\n",
    "            for img_file in os.listdir(smote_label_dir):\n",
    "                src = os.path.join(smote_label_dir, img_file)\n",
    "                dst = os.path.join(output_label_dir, img_file)\n",
    "                if not os.path.exists(dst):\n",
    "                    shutil.copy2(src, dst)\n",
    "\n",
    "    print(f\"Merged dataset created at: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "84667270",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_images_in_folders(data_dir):\n",
    "    counts = defaultdict(int)\n",
    "    \n",
    "    for label in os.listdir(data_dir):\n",
    "        label_dir = os.path.join(data_dir, label)\n",
    "        if os.path.isdir(label_dir):\n",
    "            counts[label] = len([\n",
    "                f for f in os.listdir(label_dir) \n",
    "                if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "            ])\n",
    "            \n",
    "    folder_counts = dict(counts)\n",
    "    \n",
    "    print(\"=== Image Counts by Label (From Folders) ===\")\n",
    "    for label, count in folder_counts.items():\n",
    "        print(f\"{label}: {count} images\")\n",
    "    print(f\"TOTAL: {sum(folder_counts.values())} images\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dab07a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_dataset_from_smote_csv(smote_csv_path, original_dir, output_dir, augmentations_per_image=1):\n",
    "    \"\"\"\n",
    "    Generates COMPLETELY NEW augmented dataset from SMOTE CSV\n",
    "    - Doesn't preserve any original images\n",
    "    - Creates new augmented versions for every entry in CSV\n",
    "    - Can generate multiple variations per sample (augmentations_per_image)\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    df = pd.read_csv(smote_csv_path)\n",
    "    \n",
    "    # Strong augmentation pipeline\n",
    "    aug = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.ShiftScaleRotate(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.8),\n",
    "        A.CLAHE(p=0.5),\n",
    "        A.HueSaturationValue(p=0.5),\n",
    "        A.GaussianBlur(blur_limit=(3, 7), p=0.3),\n",
    "    ])\n",
    "\n",
    "    for row_idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        # Load random original from same class\n",
    "        class_dir = os.path.join(original_dir, row['label'])\n",
    "        available_images = [f for f in os.listdir(class_dir) if f.endswith('.jpg')]\n",
    "        \n",
    "        if not available_images:\n",
    "            continue\n",
    "            \n",
    "        original_img = cv2.imread(os.path.join(class_dir, random.choice(available_images)))\n",
    "        \n",
    "        # Generate N augmented versions\n",
    "        for i in range(augmentations_per_image):\n",
    "            # Create unique filename\n",
    "            base_name = os.path.splitext(row['image_id'])[0]\n",
    "            aug_filename = f\"{base_name}_{row_idx}.jpg\"  # Using row index instead of random number\n",
    "            output_path = os.path.join(output_dir, row['label'], aug_filename)\n",
    "            \n",
    "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "            \n",
    "            # Apply augmentation and save\n",
    "            augmented = aug(image=original_img)['image']\n",
    "            cv2.imwrite(output_path, augmented)\n",
    "\n",
    "    print(f\"Generated {len(df)*augmentations_per_image} completely new augmented images in {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "89477352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_imbalance_and_augment(config):\n",
    "    \"\"\"Complete pipeline for handling imbalance and generating augmented images\"\"\"\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    df = pd.read_csv(config[\"csv_path\"])\n",
    "    le = LabelEncoder()\n",
    "    df['label_encoded'] = le.fit_transform(df['label'])\n",
    "    \n",
    "    # Split before oversampling\n",
    "    train_df, val_df = train_test_split(\n",
    "        df,\n",
    "        test_size=0.2,\n",
    "        stratify=df['label'],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Before SMOTE ===\")\n",
    "    print(train_df['label'].value_counts())\n",
    "    \n",
    "    target_counts = train_df['label'].value_counts().max()  \n",
    "    \n",
    "    balanced_dfs = []\n",
    "    for label in train_df['label'].unique():\n",
    "        label_df = train_df[train_df['label'] == label]\n",
    "        needed = target_counts - len(label_df)\n",
    "        \n",
    "        if needed > 0:\n",
    "            oversampled = label_df.sample(needed, replace=True, random_state=42)\n",
    "            balanced_dfs.append(pd.concat([label_df, oversampled]))\n",
    "        else:\n",
    "            balanced_dfs.append(label_df)\n",
    "    \n",
    "    train_df_resampled = pd.concat(balanced_dfs)\n",
    "    \n",
    "    print(\"\\n=== After Resampling ===\")\n",
    "    print(train_df_resampled['label'].value_counts())\n",
    "    \n",
    "    train_df_resampled.to_csv(config[\"balanced_train_csv\"], index=False)\n",
    "    val_df.to_csv(config[\"val_set_csv\"], index=False)\n",
    "    \n",
    "\n",
    "    generate_final_dataset_from_smote_csv(config[\"balanced_train_csv\"], config[\"original_images_dir\"], config[\"merged_output_dir\"])\n",
    "   \n",
    "    # shutil.rmtree(config[\"augmented_images_dir\"])\n",
    "    \n",
    "    print(\"\\n=== Final Counts ===\")\n",
    "    return count_images_in_folders(config[\"merged_output_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e6951667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/derrickle/anaconda3/lib/python3.12/site-packages/albumentations/core/validation.py:111: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Before SMOTE ===\n",
      "label\n",
      "normal                      1393\n",
      "blast                       1364\n",
      "hispa                       1245\n",
      "dead_heart                  1130\n",
      "tungro                       856\n",
      "brown_spot                   749\n",
      "downy_mildew                 487\n",
      "bacterial_leaf_blight        373\n",
      "bacterial_leaf_streak        294\n",
      "bacterial_panicle_blight     269\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== After Resampling ===\n",
      "label\n",
      "normal                      1393\n",
      "dead_heart                  1393\n",
      "blast                       1393\n",
      "tungro                      1393\n",
      "hispa                       1393\n",
      "bacterial_leaf_blight       1393\n",
      "brown_spot                  1393\n",
      "downy_mildew                1393\n",
      "bacterial_panicle_blight    1393\n",
      "bacterial_leaf_streak       1393\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13930/13930 [01:07<00:00, 206.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 13930 completely new augmented images in Dataset/train_images_balanced\n",
      "\n",
      "=== Final Counts ===\n",
      "=== Image Counts by Label (From Folders) ===\n",
      "normal: 1393 images\n",
      "dead_heart: 1393 images\n",
      "blast: 1393 images\n",
      "tungro: 1393 images\n",
      "hispa: 1393 images\n",
      "bacterial_leaf_blight: 1393 images\n",
      "brown_spot: 1393 images\n",
      "downy_mildew: 1393 images\n",
      "bacterial_panicle_blight: 1393 images\n",
      "bacterial_leaf_streak: 1393 images\n",
      "TOTAL: 13930 images\n",
      "\n",
      "=== Augmented Images Directory ===\n",
      "None\n",
      "\n",
      "=== Original Images Directory ===\n",
      "=== Image Counts by Label (From Folders) ===\n",
      "bacterial_leaf_blight: 479 images\n",
      "bacterial_leaf_streak: 380 images\n",
      "bacterial_panicle_blight: 337 images\n",
      "blast: 1738 images\n",
      "brown_spot: 965 images\n",
      "dead_heart: 1442 images\n",
      "downy_mildew: 620 images\n",
      "hispa: 1594 images\n",
      "normal: 1764 images\n",
      "tungro: 1088 images\n",
      "TOTAL: 10407 images\n",
      "None\n",
      "\n",
      "=== Merged Output Directory ===\n",
      "=== Image Counts by Label (From Folders) ===\n",
      "normal: 1393 images\n",
      "dead_heart: 1393 images\n",
      "blast: 1393 images\n",
      "tungro: 1393 images\n",
      "hispa: 1393 images\n",
      "bacterial_leaf_blight: 1393 images\n",
      "brown_spot: 1393 images\n",
      "downy_mildew: 1393 images\n",
      "bacterial_panicle_blight: 1393 images\n",
      "bacterial_leaf_streak: 1393 images\n",
      "TOTAL: 13930 images\n",
      "None\n",
      "\n",
      "=== Final Counts ===\n",
      "=== Image Counts by Label (From Folders) ===\n",
      "normal: 1393 images\n",
      "dead_heart: 1393 images\n",
      "blast: 1393 images\n",
      "tungro: 1393 images\n",
      "hispa: 1393 images\n",
      "bacterial_leaf_blight: 1393 images\n",
      "brown_spot: 1393 images\n",
      "downy_mildew: 1393 images\n",
      "bacterial_panicle_blight: 1393 images\n",
      "bacterial_leaf_streak: 1393 images\n",
      "TOTAL: 13930 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "merged_output_dir = handle_imbalance_and_augment(config)\n",
    "print(\"\\n=== Augmented Images Directory ===\")\n",
    "print(merged_output_dir)\n",
    "print(\"\\n=== Original Images Directory ===\")\n",
    "print(count_images_in_folders(config[\"original_images_dir\"]))\n",
    "print(\"\\n=== Merged Output Directory ===\")\n",
    "print(count_images_in_folders(config[\"merged_output_dir\"]))\n",
    "print(\"\\n=== Final Counts ===\")\n",
    "final_counts = count_images_in_folders(config[\"merged_output_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "36f1ca45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_id\n",
      "101965.jpg    12\n",
      "100058.jpg    11\n",
      "102861.jpg    10\n",
      "105100.jpg    10\n",
      "105533.jpg    10\n",
      "              ..\n",
      "109547.jpg     1\n",
      "107430.jpg     1\n",
      "108070.jpg     1\n",
      "105177.jpg     1\n",
      "100535.jpg     1\n",
      "Name: count, Length: 8160, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_resampled = pd.read_csv(config[\"balanced_train_csv\"])\n",
    "print(df_resampled['image_id'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
