{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc8e7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "config = {\n",
    "    \"csv_path\": \"processed_data/cleaned_imbalance_metadata.csv\",\n",
    "    \"train_set_csv\": \"processed_data/train_cleaned_imbalance_metadata.csv\",\n",
    "    \"val_set_csv\": \"processed_data/val_cleaned_imbalance_metadata.csv\",\n",
    "    \"label_encoder_path\": \"processed_data/le_cleaned_imbalance_metadata.npy\",\n",
    "    \"original_images_dir\": \"Dataset/train_images/\",\n",
    "    \"generated_images_dir\": \"Dataset/generated_images/\",\n",
    "    \"augmented_images_dir\": \"Dataset/augmented_images/\",\n",
    "}\n",
    "\n",
    "def load_and_preprocess_data(random_state=42):\n",
    "    df = pd.read_csv(config[\"csv_path\"])\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    df['label_encoded'] = le.fit_transform(df['label'])\n",
    "    print(f\"Label classes: {le.classes_}\")\n",
    "    \n",
    "    with open(config['label_encoder_path'], 'wb') as f:\n",
    "        np.save(f, le.classes_)\n",
    "    \n",
    "    train_df, val_df = train_test_split(\n",
    "        df, \n",
    "        test_size=0.2, \n",
    "        stratify=df['label'],\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    \n",
    "    train_df.to_csv(config['train_set_csv'], index=False)\n",
    "    val_df.to_csv(config['val_set_csv'], index=False)\n",
    "    \n",
    "    return train_df, val_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f4819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_images_based_on_csv(csv_path, src_dir, dest_dir):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        src = os.path.join(src_dir, row['label'], row['image_id'])\n",
    "        dst = os.path.join(dest_dir, row['label'], row['image_id'])\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        shutil.copy2(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c368c392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17420 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17420/17420 [00:20<00:00, 854.84it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 6125 synthetic images in Dataset/SMOT_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_augmented_images(df_resampled, original_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    aug = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.CLAHE(p=0.5),\n",
    "        A.RandomGamma(p=0.5),\n",
    "        A.HueSaturationValue(p=0.5, hue_shift_limit=20, sat_shift_limit=20, val_shift_limit=20),\n",
    "    ])\n",
    "\n",
    "    original_images = set()\n",
    "    for label in os.listdir(original_dir):\n",
    "        label_dir = os.path.join(original_dir, label)\n",
    "        if os.path.isdir(label_dir):\n",
    "            original_images.update(\n",
    "                os.path.join(label, f) \n",
    "                for f in os.listdir(label_dir) \n",
    "                if f.endswith('.jpg')\n",
    "            )\n",
    "\n",
    "    generated_count = 0\n",
    "\n",
    "    for _, row in tqdm(df_resampled.iterrows(), total=len(df_resampled)):\n",
    "        original_path = os.path.join(row['label'], row['image_id'])\n",
    "        output_path = os.path.join(output_dir, row['label'], row['image_id'])\n",
    "        \n",
    "        if original_path in original_images:\n",
    "            continue\n",
    "            \n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        class_dir = os.path.join(original_dir, row['label'])\n",
    "        class_images = [f for f in os.listdir(class_dir) if f.endswith('.jpg')]\n",
    "        if not class_images:\n",
    "            continue\n",
    "            \n",
    "        original_img = cv2.imread(os.path.join(class_dir, random.choice(class_images)))\n",
    "        \n",
    "        augmented = aug(image=original_img)['image']\n",
    "        cv2.imwrite(output_path, augmented)\n",
    "        generated_count += 1\n",
    "\n",
    "    print(f\"Generated {generated_count} synthetic images in {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeca7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(original_dir, smote_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Merges original and SMOTE-augmented images into a single dataset.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Iterate through all class subdirectories\n",
    "    for label in tqdm(os.listdir(original_dir)):\n",
    "        original_label_dir = os.path.join(original_dir, label)\n",
    "        smote_label_dir = os.path.join(smote_dir, label)\n",
    "        output_label_dir = os.path.join(output_dir, label)\n",
    "        \n",
    "        os.makedirs(output_label_dir, exist_ok=True)\n",
    "        \n",
    "        for img_file in os.listdir(original_label_dir):\n",
    "            src = os.path.join(original_label_dir, img_file)\n",
    "            dst = os.path.join(output_label_dir, img_file)\n",
    "            if not os.path.exists(dst):\n",
    "                shutil.copy2(src, dst)\n",
    "        \n",
    "        if os.path.exists(smote_label_dir):\n",
    "            for img_file in os.listdir(smote_label_dir):\n",
    "                src = os.path.join(smote_label_dir, img_file)\n",
    "                dst = os.path.join(output_label_dir, img_file)\n",
    "                if not os.path.exists(dst):\n",
    "                    shutil.copy2(src, dst)\n",
    "\n",
    "    print(f\"Merged dataset created at: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84667270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Image Counts by Label (From Folders) ===\n",
      "bacterial_leaf_blight: 1004 images\n",
      "bacterial_leaf_streak: 1075 images\n",
      "bacterial_panicle_blight: 1114 images\n",
      "blast: 24 images\n",
      "brown_spot: 584 images\n",
      "dead_heart: 225 images\n",
      "downy_mildew: 887 images\n",
      "hispa: 126 images\n",
      "tungro: 487 images\n",
      "TOTAL: 5526 images\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def count_images_in_folders(data_dir):\n",
    "    counts = defaultdict(int)\n",
    "    \n",
    "    for label in os.listdir(data_dir):\n",
    "        label_dir = os.path.join(data_dir, label)\n",
    "        if os.path.isdir(label_dir):\n",
    "            counts[label] = len([\n",
    "                f for f in os.listdir(label_dir) \n",
    "                if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "            ])\n",
    "            \n",
    "    folder_counts = dict(counts)\n",
    "    \n",
    "    print(\"=== Image Counts by Label (From Folders) ===\")\n",
    "    for label, count in folder_counts.items():\n",
    "        print(f\"{label}: {count} images\")\n",
    "    print(f\"TOTAL: {sum(folder_counts.values())} images\")\n",
    "    \n",
    "\n",
    "train_df, val_df = load_and_preprocess_data()\n",
    "df_resampled = pd.read_csv(config['train_set_csv']) \n",
    "generate_augmented_images(\n",
    "    df_resampled,\n",
    "    original_dir=config['original_images_dir'],\n",
    "    output_dir=config['generated_images_dir'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c706a6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = config['generated_images_dir']  \n",
    "count_images_in_folders(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff6a5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Image Counts by Label (From Folders) ===\n",
      "bacterial_leaf_blight: 479 images\n",
      "bacterial_leaf_streak: 380 images\n",
      "bacterial_panicle_blight: 337 images\n",
      "blast: 1738 images\n",
      "brown_spot: 965 images\n",
      "dead_heart: 1442 images\n",
      "downy_mildew: 620 images\n",
      "hispa: 1594 images\n",
      "normal: 1764 images\n",
      "tungro: 1088 images\n",
      "TOTAL: 10407 images\n"
     ]
    }
   ],
   "source": [
    "data_path = \"Dataset/train_images\"  \n",
    "count_images_in_folders(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16607d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset created at: Dataset/merged_SMOT_train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Usage\n",
    "merge_datasets(\n",
    "    original_dir=\"Dataset/train_images\",   \n",
    "    smote_dir=\"Dataset/SMOT_images\",        \n",
    "    output_dir=\"Dataset/merged_SMOT_train\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999d43c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Image Counts by Label (From Folders) ===\n",
      "bacterial_leaf_blight: 1483 images\n",
      "bacterial_leaf_streak: 1455 images\n",
      "bacterial_panicle_blight: 1451 images\n",
      "blast: 1762 images\n",
      "brown_spot: 1549 images\n",
      "dead_heart: 1667 images\n",
      "downy_mildew: 1507 images\n",
      "hispa: 1720 images\n",
      "normal: 1764 images\n",
      "tungro: 1575 images\n",
      "TOTAL: 15933 images\n"
     ]
    }
   ],
   "source": [
    "data_path = \"Dataset/merged_SMOT_train\"  \n",
    "count_images_in_folders(data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
