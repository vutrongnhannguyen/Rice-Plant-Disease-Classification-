{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48136940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Dense, Dropout, \n",
    "                                   BatchNormalization, Input, GlobalAveragePooling2D, \n",
    "                                   Concatenate)\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc98ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configuration ===\n",
    "config = {\n",
    "    \"data_path\": \"Dataset/preprocessed_images\",\n",
    "    \"csv_path\": \"processed_data/cleaned_metadata.csv\",\n",
    "    \"target_size\": (480, 320),  # 3:2 ratio\n",
    "    \"epochs\": 1,\n",
    "    \"initial_lr\": 0.001,\n",
    "    \"gpu_memory_limit\": 10,  # GB\n",
    "    \"batch_size\": 100,\n",
    "}\n",
    "\n",
    "# === GPU Setup ===\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Enable mixed precision\n",
    "        policy = mixed_precision.Policy('mixed_float16')\n",
    "        mixed_precision.set_global_policy(policy)\n",
    "        \n",
    "        # Memory growth and optimization\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.optimizer.set_jit(True)\n",
    "        tf.config.threading.set_intra_op_parallelism_threads(8)\n",
    "        tf.config.threading.set_inter_op_parallelism_threads(4)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# === Model Architecture ===\n",
    "def create_gpu_optimized_model(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape, dtype=tf.float16)\n",
    "    \n",
    "    # Initial feature extraction\n",
    "    x = Conv2D(96, (7,7), strides=2, activation='relu', padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((3,3), strides=2)(x)\n",
    "    \n",
    "    # Intermediate layers\n",
    "    x = Conv2D(256, (5,5), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((3,3), strides=2)(x)\n",
    "    \n",
    "    # Parallel paths\n",
    "    branch1 = Conv2D(384, (3,3), activation='relu', padding='same')(x)\n",
    "    branch2 = Conv2D(384, (3,3), dilation_rate=2, activation='relu', padding='same')(x)\n",
    "    x = Concatenate()([branch1, branch2])\n",
    "    \n",
    "    # Final classification head\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, activation='relu', name='features')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax', dtype=tf.float32)(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# === Memory Management ===\n",
    "def calculate_max_batch_size(model, input_shape=(480, 320, 3), gpu_mem=24):\n",
    "    \"\"\"Improved batch size calculator\"\"\"\n",
    "    try:\n",
    "        # Estimate memory usage per sample\n",
    "        params = model.count_params()\n",
    "        \n",
    "        # Find last dense layer\n",
    "        last_dense = next(layer for layer in reversed(model.layers) \n",
    "                     if isinstance(layer, Dense) and layer.name == 'features')\n",
    "        \n",
    "        # Memory estimation (conservative)\n",
    "        per_sample = (params * 4 +  # 4 bytes per parameter\n",
    "                     np.prod(input_shape) * last_dense.units * 4) / (1024**3)  # GB\n",
    "        \n",
    "        # Calculate max batch with 3GB buffer\n",
    "        max_batch = int((gpu_mem - 3) / per_sample)\n",
    "        \n",
    "        return max(16, min(256, max_batch))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating batch size: {e}\")\n",
    "        return 32  # Fallback value\n",
    "\n",
    "def cleanup_gpu_memory():\n",
    "    \"\"\"Force clear GPU memory\"\"\"\n",
    "    K.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    if tf.config.list_physical_devices('GPU'):\n",
    "        try:\n",
    "            for gpu in tf.config.list_physical_devices('GPU'):\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "\n",
    "# === Data Pipeline ===\n",
    "def load_and_preprocess_data(random_state=42, save_splits=True):\n",
    "    \"\"\"Load and split data with fixed random state for reproducibility\"\"\"\n",
    "    df = pd.read_csv(config[\"csv_path\"])\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    df['label_encoded'] = le.fit_transform(df['label'])\n",
    "    \n",
    "    # Save label encoder classes\n",
    "    with open('training6_label_encoder.npy', 'wb') as f:\n",
    "        np.save(f, le.classes_)\n",
    "    \n",
    "    # Split data\n",
    "    train_df, val_df = train_test_split(\n",
    "        df, \n",
    "        test_size=0.15, \n",
    "        stratify=df['label'],\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Save splits for later reference\n",
    "    if save_splits:\n",
    "        train_df.to_csv('training6_train_set.csv', index=False)\n",
    "        val_df.to_csv('training6_validation_set.csv', index=False)\n",
    "    \n",
    "    return train_df, val_df, le\n",
    "\n",
    "class RiceDataGenerator(Sequence):\n",
    "    def __init__(self, df, base_path, batch_size=32, target_size=(480, 320), shuffle=True, debug=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.base_path = base_path\n",
    "        self.batch_size = batch_size\n",
    "        self.target_size = target_size  \n",
    "        self.shuffle = shuffle\n",
    "        self.debug = debug\n",
    "        self.indices = np.arange(len(df))\n",
    "        \n",
    "        self.aug = A.Compose([\n",
    "            # A.RandomRotate90(),\n",
    "            # A.HorizontalFlip(),\n",
    "            # A.VerticalFlip(),\n",
    "            # A.Transpose(),\n",
    "            # A.RandomBrightnessContrast(p=0.5),\n",
    "            # A.HueSaturationValue(p=0.5),\n",
    "            # A.CLAHE(p=0.5),\n",
    "            A.Resize(width=self.target_size[0], height=self.target_size[1]),\n",
    "        ])\n",
    "        \n",
    "        if shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "        if self.debug:\n",
    "            self._visualize_samples()    \n",
    "            \n",
    "\n",
    "    def _visualize_samples(self):\n",
    "        \"\"\"Visualize first 2 samples after augmentation\"\"\"\n",
    "        \n",
    "        for i in range(min(2, len(self.df))):\n",
    "            try:\n",
    "                row = self.df.iloc[i]\n",
    "                img = self._load_image(row['image_id'], row['label'])\n",
    "                augmented = self.aug(image=img)\n",
    "                \n",
    "                plt.figure(figsize=(12, 6))\n",
    "                \n",
    "                # Show original\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.imshow(img)\n",
    "                plt.title(f\"Original\\nShape: {img.shape}\")\n",
    "                \n",
    "                # Show augmented\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.imshow(augmented['image'])\n",
    "                plt.title(f\"Augmented\\nShape: {augmented['image'].shape}\")\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Visualization failed for {row['image_id']}: {str(e)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / self.batch_size))\n",
    "    \n",
    "    @property\n",
    "    def num_batches(self):\n",
    "        return len(self)\n",
    "    \n",
    "    def _load_image(self, image_id, label, suffix='green'):\n",
    "        img_path = os.path.join(\n",
    "            self.base_path,\n",
    "            label,\n",
    "            f\"{os.path.splitext(image_id)[0]}_{suffix}.jpg\"\n",
    "        )\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(f\"Image not found at {img_path}\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        # Resize using (width, height)\n",
    "        img = cv2.resize(img, self.target_size)  \n",
    "        return img\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        batch_df = self.df.iloc[batch_indices]\n",
    "        \n",
    "        # Initialize with correct dimensions (height, width, channels)\n",
    "        X = np.zeros((len(batch_df), self.target_size[1], self.target_size[0], 3), dtype=np.float32)\n",
    "        y = np.zeros((len(batch_df),), dtype=np.int32)\n",
    "        \n",
    "        for i, (_, row) in enumerate(batch_df.iterrows()):\n",
    "            try:\n",
    "                img = self._load_image(row['image_id'], row['label'])\n",
    "                # No need to transpose - we'll match model to data shape\n",
    "                augmented = self.aug(image=img)\n",
    "                X[i] = augmented['image'] / 255.0\n",
    "                y[i] = row['label_encoded']\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {row['image_id']}: {str(e)}\")\n",
    "                X[i] = np.zeros((self.target_size[1], self.target_size[0], 3), dtype=np.float32)\n",
    "                y[i] = -1\n",
    "                \n",
    "        valid = y != -1\n",
    "        return X[valid], y[valid]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Shuffle indices after each epoch\"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "# === Training ===\n",
    "def train():\n",
    "    cleanup_gpu_memory()\n",
    "    \n",
    "    try:\n",
    "        # Load data with fixed random state\n",
    "        train_df, val_df, le = load_and_preprocess_data(random_state=42)\n",
    "        num_classes = len(le.classes_)\n",
    "        \n",
    "        # Create model\n",
    "        input_shape = (config[\"target_size\"][1], config[\"target_size\"][0], 3)\n",
    "        model = create_gpu_optimized_model(input_shape, num_classes)\n",
    "        \n",
    "        # Calculate batch size\n",
    "        cleanup_gpu_memory()\n",
    "        batch_size = calculate_max_batch_size(model)\n",
    "        \n",
    "        print(f\"\\n=== Training Configuration ===\")\n",
    "        print(f\"Batch size: {batch_size}\")\n",
    "        print(f\"Input size: {config['target_size']}\")\n",
    "        print(f\"Classes: {num_classes}\")\n",
    "        print(f\"GPU Memory: {config['gpu_memory_limit']}GB\\n\")\n",
    "        print(f\"Model input shape: {model.input_shape}\")\n",
    "        \n",
    "        # Create generators\n",
    "        train_gen = RiceDataGenerator(\n",
    "            df=train_df,\n",
    "            base_path=config[\"data_path\"],\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            target_size=config[\"target_size\"],\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        val_gen = RiceDataGenerator(\n",
    "            df=val_df,\n",
    "            base_path=config[\"data_path\"],\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            target_size=config[\"target_size\"],\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=AdamW(learning_rate=config[\"initial_lr\"], weight_decay=1e-4),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        history = model.fit(\n",
    "            train_gen,\n",
    "            validation_data=val_gen,\n",
    "            epochs=config[\"epochs\"],\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "                tf.keras.callbacks.ModelCheckpoint(\n",
    "                    'training6_best_model.h5',\n",
    "                    save_best_only=True,\n",
    "                    monitor='val_accuracy'\n",
    "                ),\n",
    "                tf.keras.callbacks.CSVLogger('training6_history.csv')\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return model, history\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {e}\")\n",
    "        cleanup_gpu_memory()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23648db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_saved_model(model_path, use_val_set=True):\n",
    "    \"\"\"Evaluate a saved model with comprehensive metrics\"\"\"\n",
    "    try:\n",
    "        # Load label encoder\n",
    "        with open('training6_label_encoder.npy', 'rb') as f:\n",
    "            classes = np.load(f, allow_pickle=True)\n",
    "        le = LabelEncoder()\n",
    "        le.classes_ = classes\n",
    "        \n",
    "        # Load model with custom objects\n",
    "        custom_objects = {'AdamW': tf.keras.optimizers.AdamW}\n",
    "        with tf.keras.utils.custom_object_scope(custom_objects):\n",
    "            model = tf.keras.models.load_model(model_path)\n",
    "        \n",
    "        # Load appropriate dataset\n",
    "        if use_val_set:\n",
    "            print(\"\\nUsing saved validation set for evaluation\")\n",
    "            eval_df = pd.read_csv('training6_validation_set.csv')\n",
    "        else:\n",
    "            print(\"\\nUsing new test data for evaluation\")\n",
    "            _, eval_df, _ = load_and_preprocess_data(save_splits=False)\n",
    "        \n",
    "        # Create evaluation generator\n",
    "        eval_gen = RiceDataGenerator(\n",
    "            df=eval_df,\n",
    "            base_path=config[\"data_path\"],\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            target_size=config[\"target_size\"],\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        # Recompile with proper metrics\n",
    "        model.compile(\n",
    "            optimizer=model.optimizer,\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=[\n",
    "                'accuracy',\n",
    "                tf.keras.metrics.SparseAUC(name='auc'),\n",
    "                tf.keras.metrics.SparsePrecision(name='precision'),\n",
    "                tf.keras.metrics.SparseRecall(name='recall'),\n",
    "                tf.keras.metrics.SparseTopKCategoricalAccuracy(k=3, name='top3_accuracy')\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        print(\"\\n=== Evaluating Model ===\")\n",
    "        results = model.evaluate(eval_gen, verbose=1)\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n=== Evaluation Results ===\")\n",
    "        for name, value in zip(model.metrics_names, results):\n",
    "            print(f\"{name:15}: {value:.4f}\")\n",
    "        \n",
    "        # Generate predictions\n",
    "        print(\"\\nGenerating predictions...\")\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        for i in range(len(eval_gen)):\n",
    "            x, y = eval_gen[i]\n",
    "            y_true.extend(y)\n",
    "            y_pred.extend(model.predict(x, verbose=0).argmax(axis=1))\n",
    "        \n",
    "        plot_confusion_matrix(np.array(y_true), np.array(y_pred), le.classes_)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation failed: {e}\")\n",
    "        cleanup_gpu_memory()\n",
    "        raise\n",
    "    \n",
    "def plot_confusion_matrix(y_true, y_pred, classes):\n",
    "    \"\"\"Plot a detailed confusion matrix\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot(include_values=True, ax=ax, cmap='viridis',\n",
    "              xticks_rotation='vertical', values_format='d')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415ac2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Execution Options ===\n",
    "if __name__ == \"__main__\":\n",
    "    # Option 1: Train and evaluate\n",
    "    model, history = train()\n",
    "    evaluate_saved_model('training6_best_model.h5')\n",
    "    \n",
    "    # Option 2: Only evaluate a previously trained model\n",
    "    # evaluate_saved_model('training6_best_model.h5', \n",
    "    #                     test_data_path=\"path/to/test/images\",\n",
    "    #                     test_csv_path=\"path/to/test_metadata.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
