{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790becaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from cleanvision import Imagelab\n",
    "from cv2.typing import MatLike\n",
    "\n",
    "df = pd.read_csv(\"Dataset/meta_train.csv\")\n",
    "\n",
    "print(\"Initial shape of the dataset: \", df.shape)\n",
    "\n",
    "imagelab = Imagelab(data_path=\"Dataset/train_images\")\n",
    "imagelab.find_issues()\n",
    "\n",
    "imagelab.info[\"statistics\"][\"aspect_ratio\"]\n",
    "\n",
    "imagelab.info[\"statistics\"][\"size\"]\n",
    "\n",
    "imagelab.issue_summary\n",
    "\n",
    "exact_duplicate = imagelab.issues[\n",
    "    imagelab.issues[\"is_exact_duplicates_issue\"]\n",
    "].index.tolist()\n",
    "near_duplicate = imagelab.issues[\n",
    "    imagelab.issues[\"is_near_duplicates_issue\"]\n",
    "].index.tolist()\n",
    "imagelab.visualize(image_files=exact_duplicate[:8])\n",
    "imagelab.visualize(image_files=near_duplicate[:8])\n",
    "\n",
    "unreliable_sets = []\n",
    "\n",
    "for set in imagelab.info[\"exact_duplicates\"][\"sets\"]:\n",
    "    image_1, image_2 = [set[0].split(\"/\")[-1], set[1].split(\"/\")[-1]]\n",
    "    meta_1 = (\n",
    "        df[df[\"image_id\"] == image_1].reset_index().drop([\"image_id\", \"index\"], axis=1)\n",
    "    )\n",
    "    meta_2 = (\n",
    "        df[df[\"image_id\"] == image_2].reset_index().drop([\"image_id\", \"index\"], axis=1)\n",
    "    )\n",
    "    # if comparision has value\n",
    "    if not meta_1.compare(meta_2).empty:\n",
    "        unreliable_sets.append(set)\n",
    "        print(\"different metadata\")\n",
    "\n",
    "for set in imagelab.info[\"near_duplicates\"][\"sets\"]:\n",
    "    image_1, image_2 = [set[0].split(\"/\")[-1], set[1].split(\"/\")[-1]]\n",
    "    meta_1 = (\n",
    "        df[df[\"image_id\"] == image_1].reset_index().drop([\"image_id\", \"index\"], axis=1)\n",
    "    )\n",
    "    meta_2 = (\n",
    "        df[df[\"image_id\"] == image_2].reset_index().drop([\"image_id\", \"index\"], axis=1)\n",
    "    )\n",
    "    # if comparision has value\n",
    "    if not meta_1.compare(meta_2).empty:\n",
    "        unreliable_sets.append(set)\n",
    "        print(\"different metadata\")\n",
    "\n",
    "for set in imagelab.info[\"exact_duplicates\"][\"sets\"]:\n",
    "    for i in range(len(set)):\n",
    "        if i != 0:\n",
    "            df = df[df[\"image_id\"] != set[i].split(\"/\")[-1]]\n",
    "\n",
    "for set in imagelab.info[\"near_duplicates\"][\"sets\"]:\n",
    "    for i in range(len(set)):\n",
    "        if i != 0:\n",
    "            df = df[df[\"image_id\"] != set[i].split(\"/\")[-1]]\n",
    "\n",
    "for set in unreliable_sets:\n",
    "    for i in range(len(set)):\n",
    "        df = df[df[\"image_id\"] != set[i].split(\"/\")[-1]]\n",
    "        \n",
    "print(\"Final shape of the dataset: \", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc98ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import mixed_precision\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Input, GlobalAveragePooling2D, Concatenate\n",
    "\n",
    "def create_gpu_optimized_model(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape, dtype=tf.float16)\n",
    "    \n",
    "    # Large kernel initial layers to utilize GPU parallelism\n",
    "    x = Conv2D(96, (11,11), strides=4, activation='relu', padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((3,3), strides=2)(x)\n",
    "    \n",
    "    # Bottleneck layers for memory efficiency\n",
    "    x = Conv2D(256, (5,5), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((3,3), strides=2)(x)\n",
    "    \n",
    "    # Parallel convolutions\n",
    "    branch1 = Conv2D(384, (3,3), activation='relu', padding='same')(x)\n",
    "    branch2 = Conv2D(384, (3,3), dilation_rate=2, activation='relu', padding='same')(x)\n",
    "    x = Concatenate()([branch1, branch2])\n",
    "    \n",
    "    # Final layers\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax', dtype=tf.float32)(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff3b6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_max_batch_size(model, input_shape=(480, 320, 3), gpu_mem=24):\n",
    "    \"\"\"Calculate maximum batch size for 24GB VRAM\"\"\"\n",
    "    from keras import backend as K\n",
    "    import numpy as np\n",
    "    \n",
    "    # Memory usage per sample (empirical formula)\n",
    "    params = model.count_params()\n",
    "    activations = np.prod(input_shape) * model.layers[-2].units  # Last dense layer\n",
    "    per_sample = (params * 2 + activations * 4) / (1024 ** 3)  # GB\n",
    "    \n",
    "    # Available VRAM (with 2GB buffer)\n",
    "    usable_mem = gpu_mem - 2\n",
    "    \n",
    "    max_batch = int(usable_mem / per_sample)\n",
    "    return max(16, min(256, max_batch))  # Keep within reasonable bounds\n",
    "\n",
    "# Example usage:\n",
    "model = create_gpu_optimized_model((480, 320, 3), num_classes=10)\n",
    "recommended_batch = calculate_max_batch_size(model)\n",
    "print(f\"Recommended batch size: {recommended_batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be211517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# === Configuration ===\n",
    "config = {\n",
    "    \"data_path\": \"Dataset/train_images\",\n",
    "    \"csv_path\": \"Dataset/meta_train.csv\",\n",
    "    \"target_size\": (480, 320),  # 3:2 ratio\n",
    "    \"epochs\": 100,\n",
    "    \"initial_lr\": 0.001,\n",
    "    \"gpu_memory_limit\": 24  # GB\n",
    "}\n",
    "\n",
    "# === GPU Setup ===\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Enable mixed precision\n",
    "        policy = mixed_precision.Policy('mixed_float16')\n",
    "        mixed_precision.set_global_policy(policy)\n",
    "        \n",
    "        # Memory growth and optimization\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.optimizer.set_jit(True)\n",
    "        tf.config.threading.set_intra_op_parallelism_threads(8)\n",
    "        tf.config.threading.set_inter_op_parallelism_threads(4)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# === Data Preparation ===\n",
    "def load_and_preprocess_data():\n",
    "    df = pd.read_csv(config[\"csv_path\"])\n",
    "    \n",
    "    # Label encoding\n",
    "    le = LabelEncoder()\n",
    "    df['label_encoded'] = le.fit_transform(df['label'])\n",
    "    \n",
    "    # Train-val split\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_df, val_df = train_test_split(df, test_size=0.15, stratify=df['label'])\n",
    "    \n",
    "    return train_df, val_df, len(le.classes_)\n",
    "\n",
    "# === Data Generator ===\n",
    "class RiceDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, df, base_path, batch_size=32, shuffle=True):\n",
    "        self.df = df\n",
    "        self.base_path = base_path\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(df))\n",
    "        self.aug = A.Compose([\n",
    "            A.RandomRotate90(),\n",
    "            A.Flip(),\n",
    "            A.Transpose(),\n",
    "            A.RandomBrightnessContrast(p=0.5),\n",
    "            A.HueSaturationValue(p=0.5),\n",
    "            A.CLAHE(p=0.5),\n",
    "        ])\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        batch_df = self.df.iloc[batch_indices]\n",
    "        \n",
    "        X = np.zeros((len(batch_df), *config[\"target_size\"], 3), dtype=np.float16)\n",
    "        y = np.zeros((len(batch_df),), dtype=np.int32)\n",
    "        \n",
    "        for i, (_, row) in enumerate(batch_df.iterrows()):\n",
    "            img = cv2.imread(os.path.join(self.base_path, row['image_id']))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = self.aug(image=img)['image']\n",
    "            X[i] = img.astype(np.float16) / 255.0\n",
    "            y[i] = row['label_encoded']\n",
    "            \n",
    "        return X, y\n",
    "\n",
    "# === Model Training ===\n",
    "def train():\n",
    "    # Load data\n",
    "    train_df, val_df, num_classes = load_and_preprocess_data()\n",
    "    \n",
    "    # Create model\n",
    "    model = create_gpu_optimized_model((*config[\"target_size\"], 3), num_classes)\n",
    "    \n",
    "    # Calculate optimal batch size\n",
    "    batch_size = calculate_max_batch_size(model)\n",
    "    print(f\"\\n=== Training Configuration ===\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Input size: {config['target_size']}\")\n",
    "    print(f\"GPU Memory: {config['gpu_memory_limit']}GB\\n\")\n",
    "    \n",
    "    # Create generators\n",
    "    train_gen = RiceDataGenerator(train_df, config[\"data_path\"], batch_size=batch_size)\n",
    "    val_gen = RiceDataGenerator(val_df, config[\"data_path\"], batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.AdamW(learning_rate=config[\"initial_lr\"], weight_decay=1e-4),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy',\n",
    "                 tf.keras.metrics.AUC(name='auc'),\n",
    "                 tf.keras.metrics.Precision(name='precision'),\n",
    "                 tf.keras.metrics.Recall(name='recall')]\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3),\n",
    "        tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True),\n",
    "        tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "    ]\n",
    "    \n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=config[\"epochs\"],\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# === Execution ===\n",
    "if __name__ == \"__main__\":\n",
    "    model, history = train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
