{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d2a5194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 02:51:19.849905: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-15 02:51:19.853551: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-15 02:51:19.943399: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-15 02:51:19.976990: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747252280.039372    6538 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747252280.052408    6538 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747252280.155142    6538 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747252280.155162    6538 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747252280.155163    6538 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747252280.155164    6538 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-15 02:51:20.159887: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Dense, Dropout, \n",
    "                                   BatchNormalization, Input, GlobalAveragePooling2D, \n",
    "                                   Concatenate, Multiply, Reshape)\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import albumentations as A\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "# %pip install optuna \n",
    "# %pip install optuna-integration[tfkeras]\n",
    "import optuna\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "\n",
    "\n",
    "#*** Model Save is disanbled for testing purposes ***\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices=false'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  \n",
    "\n",
    "# === Configuration ===\n",
    "config = {\n",
    "    \"epochs\": 1,\n",
    "    \"is_config_batch_size_param\": True,\n",
    "    \"batch_size\": 200,\n",
    "    \"initial_lr\": 0.001,\n",
    "    \"gpu_memory_limit\": 45,\n",
    "    \"target_size\": (480, 640),  # 2:3 ratio (width, height)\n",
    "    \"input_shape\": (640, 480, 3), # (height, width, channels) for Keras\n",
    "    \"data_path\": \"Dataset/merged_SMOT_train\",\n",
    "    \"csv_path\": \"processed_data/cleaned_metadata_short.csv\",\n",
    "    \"train_set_csv\": \"Model/training8_customCNN_rgb_att_SMOT_aug_bay/training8_customCNN_rgb_att_SMOT_aug_bay_train_set.csv\",\n",
    "    \"val_set_csv\": \"Model/training8_customCNN_rgb_att_SMOT_aug_bay/training8_customCNN_rgb_att_SMOT_aug_bay_validation_set.csv\",\n",
    "    \"history_csv\": \"Model/training8_customCNN_rgb_att_SMOT_aug_bay/training8_customCNN_rgb_att_SMOT_aug_bay_history.csv\",\n",
    "    \"best_model\": \"Model/training8_customCNN_rgb_att_SMOT_aug_bay/training8_customCNN_rgb_att_SMOT_aug_bay_best_model.keras\",\n",
    "    \"label_encoder_path\": \"Model/training8_customCNN_rgb_att_SMOT_aug_bay/training8_customCNN_rgb_att_SMOT_aug_bay_label_encoder.npy\",\n",
    "    \"color_channel\": \"\",\n",
    "    \"save_dir\": \"Model/training8_customCNN_rgb_att_SMOT_aug_bay\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8d9d202",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 02:51:23.470884: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "# === GPU Setup ===\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        policy = mixed_precision.Policy('float32')\n",
    "        mixed_precision.set_global_policy(policy)\n",
    "        \n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.optimizer.set_jit(True)\n",
    "        tf.config.threading.set_intra_op_parallelism_threads(8)\n",
    "        tf.config.threading.set_inter_op_parallelism_threads(4)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# === Memory Management ===\n",
    "def calculate_max_batch_size(model, input_shape, gpu_mem=24, default_batch=32, is_use_config_batch_size=config[\"is_config_batch_size_param\"]):\n",
    "    \"\"\"Improved batch size calculator with error handling\"\"\"\n",
    "    if is_use_config_batch_size:\n",
    "        return default_batch\n",
    "    try:\n",
    "        params = model.count_params()\n",
    "        \n",
    "        last_dense = None\n",
    "        for layer in reversed(model.layers):\n",
    "            if isinstance(layer, tf.keras.layers.Dense):\n",
    "                last_dense = layer\n",
    "                if layer.name == 'features':  \n",
    "                    break\n",
    "        \n",
    "        if last_dense is None:\n",
    "            raise ValueError(\"No Dense layer found in model!\")\n",
    "        \n",
    "        # Memory per sample \n",
    "        per_sample = (\n",
    "            (params * 4) +                 \n",
    "            (np.prod(input_shape) * last_dense.units * 4)  \n",
    "        ) / (1024 ** 3)\n",
    "        \n",
    "        max_batch = int((gpu_mem - 3) / per_sample)\n",
    "        return min(256, max_batch)  \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Batch size estimation failed, using default={default_batch}. Error: {e}\")\n",
    "        return default_batch\n",
    "\n",
    "def cleanup_gpu_memory():\n",
    "    \"\"\"Force clear GPU memory\"\"\"\n",
    "    K.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    if tf.config.list_physical_devices('GPU'):\n",
    "        try:\n",
    "            for gpu in tf.config.list_physical_devices('GPU'):\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "\n",
    "# === Data Pipeline ===\n",
    "def load_and_preprocess_data(random_state=42, save_splits=True):\n",
    "    \"\"\"Load and split data with fixed random state for reproducibility\"\"\"\n",
    "    df = pd.read_csv(config[\"csv_path\"])\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    df['label_encoded'] = le.fit_transform(df['label'])\n",
    "    print(f\"Label classes: {le.classes_}\")\n",
    "    \n",
    "    with open(config['label_encoder_path'], 'wb') as f:\n",
    "        np.save(f, le.classes_)\n",
    "    \n",
    "    train_df, val_df = train_test_split(\n",
    "        df, \n",
    "        test_size=0.2, \n",
    "        stratify=df['label'],\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    \n",
    "    if save_splits:\n",
    "        train_df.to_csv(config['train_set_csv'], index=False)\n",
    "        val_df.to_csv(config['val_set_csv'], index=False)\n",
    "    \n",
    "    return train_df, val_df, le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92577bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiceDataGenerator(Sequence):\n",
    "    def __init__(self, df, base_path, batch_size=32, target_size=(480, 640), shuffle=False, debug=False, config=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.base_path = base_path\n",
    "        self.batch_size = batch_size\n",
    "        self.target_size = target_size  \n",
    "        self.shuffle = shuffle\n",
    "        self.debug = debug\n",
    "        self.indices = np.arange(len(df))\n",
    "        self.config = config if config else {}\n",
    "        \n",
    "        self.aug = A.Compose(config[\"augmentation\"])\n",
    "        \n",
    "        if shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "        if self.debug:\n",
    "            self.visualize_samples()    \n",
    "            \n",
    "\n",
    "    def visualize_samples(self):        \n",
    "        try:\n",
    "            row = self.df.iloc[0]\n",
    "            img = self._load_image(row['image_id'], row['label'])\n",
    "            augmented = self.aug(image=img)\n",
    "            \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            # original\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"Original\\nShape: {img.shape}\")\n",
    "            \n",
    "            # augmented\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.imshow(augmented['image'])\n",
    "            plt.title(f\"Augmented\\nShape: {augmented['image'].shape}\")\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Visualization failed for {row['image_id']}: {str(e)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / self.batch_size))\n",
    "    \n",
    "    def _load_image(self, image_id, label):\n",
    "        img_path = os.path.join(\n",
    "            self.base_path,\n",
    "            label,\n",
    "            f\"{os.path.splitext(image_id)[0]}.jpg\"\n",
    "        )\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(f\"Image not found at {img_path}\")\n",
    "        return img\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        batch_df = self.df.iloc[batch_indices]\n",
    "        \n",
    "        X = np.zeros((len(batch_df), self.target_size[1], self.target_size[0], 3), dtype=np.float32) #(batch, height, width, channels)\n",
    "        y = np.zeros((len(batch_df),), dtype=np.int32)\n",
    "        \n",
    "        for i, (_, row) in enumerate(batch_df.iterrows()):\n",
    "            try:\n",
    "                img = self._load_image(row['image_id'], row['label'])\n",
    "                augmented = self.aug(image=img)\n",
    "                X[i] = augmented['image'] / 255.0\n",
    "                y[i] = row['label_encoded']\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {row['image_id']}: {str(e)}\")\n",
    "                X[i] = np.zeros((self.target_size[1], self.target_size[0], 3), dtype=np.float32) #(batch, height, width, channels)\n",
    "                y[i] = -1\n",
    "                \n",
    "        valid = y != -1\n",
    "        return X[valid], y[valid]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dacc4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Model Architecture ===\n",
    "def se_block(input_tensor, ratio=16):\n",
    "    channels = input_tensor.shape[-1]\n",
    "    se = GlobalAveragePooling2D()(input_tensor)\n",
    "    se = Dense(channels // ratio, activation=\"relu\")(se)\n",
    "    se = Dense(channels, activation=\"sigmoid\")(se)\n",
    "    return Multiply()([input_tensor, se])\n",
    "\n",
    "def create_customCNN(input_shape, num_classes):    \n",
    "    inputs = Input(shape=input_shape, dtype=tf.float32) \n",
    "     \n",
    "    # Initial feature extraction\n",
    "    x = Conv2D(96, (7,7), strides=2, activation='relu', padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((3,3), strides=2)(x)\n",
    "    \n",
    "    x = se_block(x)\n",
    "\n",
    "    # Intermediate layers\n",
    "    x = Conv2D(256, (5,5), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((3,3), strides=2)(x)\n",
    "    \n",
    "    # Parallel paths\n",
    "    branch1 = Conv2D(384, (3,3), activation='relu', padding='same')(x)\n",
    "    branch2 = Conv2D(384, (3,3), dilation_rate=2, activation='relu', padding='same')(x)\n",
    "    x = Concatenate()([branch1, branch2])\n",
    "        \n",
    "    # Final classification head\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, activation='relu', name='features')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax', dtype=tf.float32)(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbfeea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Training ===\n",
    "def train(config=None):\n",
    "    cleanup_gpu_memory()\n",
    "    \n",
    "    try:\n",
    "        train_df, val_df, le = load_and_preprocess_data(random_state=42)\n",
    "        num_classes = len(le.classes_)\n",
    "        print(\"Classes: \", num_classes)\n",
    "        \n",
    "        # Create the mopdel\n",
    "        input_shape = config[\"input_shape\"] \n",
    "        model = create_customCNN(input_shape, num_classes)\n",
    "        \n",
    "        # Find optimal batch size\n",
    "        cleanup_gpu_memory()\n",
    "        optimized_batch_size = calculate_max_batch_size(\n",
    "                                    model, \n",
    "                                    input_shape=config[\"input_shape\"],\n",
    "                                    gpu_mem=config[\"gpu_memory_limit\"],\n",
    "                                    default_batch=config[\"batch_size\"],\n",
    "                                )\n",
    "        \n",
    "        print(f\"\\n=== Training Configuration ===\")\n",
    "        print(f\"Batch size: {optimized_batch_size}\")\n",
    "        print(f\"Input size: {config['target_size']}\")\n",
    "        print(f\"Classes: {num_classes}\")\n",
    "        print(f\"GPU Memory: {config['gpu_memory_limit']}GB\\n\")\n",
    "        print(f\"Model input shape: {model.input_shape}\")\n",
    "        \n",
    "        # Create generators for training and validation\n",
    "        train_gen = RiceDataGenerator(\n",
    "            df=train_df,\n",
    "            base_path=config[\"data_path\"],\n",
    "            batch_size=optimized_batch_size,\n",
    "            target_size=config[\"target_size\"],\n",
    "            shuffle=False,\n",
    "            debug=True,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        val_gen = RiceDataGenerator(\n",
    "            df=val_df,\n",
    "            base_path=config[\"data_path\"],\n",
    "            batch_size=optimized_batch_size,\n",
    "            target_size=config[\"target_size\"],\n",
    "            shuffle=False,\n",
    "            debug=False,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        sample_batch = train_gen[0]\n",
    "        print(f\"Generator output shape: {sample_batch[0].shape}\")\n",
    "        print(f\"Model input shape: {model.input_shape}\")\n",
    "        \n",
    "        # Train\n",
    "        history = model.fit(\n",
    "            train_gen,\n",
    "            validation_data=val_gen,\n",
    "            epochs=config[\"epochs\"],\n",
    "            callbacks=[\n",
    "                    tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_accuracy',\n",
    "                    patience=5, \n",
    "                    mode='max',\n",
    "                    restore_best_weights=True  \n",
    "                ),\n",
    "                tf.keras.callbacks.ModelCheckpoint(\n",
    "                    config[\"best_model\"],  \n",
    "                    save_weights_only=False,\n",
    "                    monitor='val_accuracy',\n",
    "                    save_best_only=True \n",
    "                ),\n",
    "                    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                    monitor='val_accuracy',\n",
    "                    factor=0.5,  # Halve Learn Rate\n",
    "                    patience=3,\n",
    "                    mode='max'\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Plot accuracy\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "        plt.axhline(y=max(history.history['val_accuracy']), color='r', linestyle='--', label='Best Val Accuracy')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot loss\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "        plt.axhline(y=min(history.history['val_loss']), color='r', linestyle='--', label='Best Val Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return model, history\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {e}\")\n",
    "        cleanup_gpu_memory()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f769214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to optimize\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.3, 0.7)\n",
    "    \n",
    "    # Update config with suggested values\n",
    "    config.update({\n",
    "        \"initial_lr\": lr,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "    })\n",
    "    \n",
    "    # Load data and train (use your existing `train()` function)\n",
    "    cleanup_gpu_memory()\n",
    "    model, history = train(config)\n",
    "    \n",
    "    # Return validation accuracy (what we want to maximize)\n",
    "    return max(history.history[\"val_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8629ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_objective(config, train_df, val_df, num_classes):\n",
    "    def objective(trial):\n",
    "        # Hyperparameters to optimize\n",
    "        lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "        dropout_rate = trial.suggest_float(\"dropout_rate\", 0.3, 0.7)\n",
    "        conv_filters = trial.suggest_categorical(\"conv_filters\", [64, 96, 128])\n",
    "        \n",
    "        # Update config with suggested values\n",
    "        current_config = config.copy()\n",
    "        current_config.update({\n",
    "            \"initial_lr\": lr,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"dropout_rate\": dropout_rate,\n",
    "        })\n",
    "        \n",
    "        # Create model with dynamic architecture\n",
    "        def create_model():\n",
    "            inputs = Input(shape=current_config[\"input_shape\"], dtype=tf.float32)\n",
    "            x = Conv2D(conv_filters, (7,7), strides=2, activation='relu', padding='same')(inputs)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = MaxPooling2D((3,3), strides=2)(x)\n",
    "            x = se_block(x)\n",
    "\n",
    "            # Intermediate layers\n",
    "            x = Conv2D(256, (5,5), activation='relu', padding='same')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = MaxPooling2D((3,3), strides=2)(x)\n",
    "            \n",
    "            # Parallel paths\n",
    "            branch1 = Conv2D(384, (3,3), activation='relu', padding='same')(x)\n",
    "            branch2 = Conv2D(384, (3,3), dilation_rate=2, activation='relu', padding='same')(x)\n",
    "            x = Concatenate()([branch1, branch2])\n",
    "                \n",
    "            # Final classification head\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "            x = Dense(512, activation='relu', name='features')(x)\n",
    "            x = Dropout(0.5)(x)\n",
    "            outputs = Dense(num_classes, activation='softmax', dtype=tf.float32)(x)\n",
    "            \n",
    "            return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "        # Create data generators\n",
    "        train_gen = RiceDataGenerator(\n",
    "            df=train_df,\n",
    "            base_path=current_config[\"data_path\"],\n",
    "            batch_size=batch_size,\n",
    "            target_size=current_config[\"target_size\"],\n",
    "            config=current_config\n",
    "        )\n",
    "        \n",
    "        val_gen = RiceDataGenerator(\n",
    "            df=val_df,\n",
    "            base_path=current_config[\"data_path\"],\n",
    "            batch_size=batch_size,\n",
    "            target_size=current_config[\"target_size\"],\n",
    "            config=current_config\n",
    "        )\n",
    "        \n",
    "        # Create and compile model\n",
    "        model = create_model()\n",
    "        model.compile(\n",
    "            optimizer=AdamW(learning_rate=lr),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Train with pruning\n",
    "        history = model.fit(\n",
    "            train_gen,\n",
    "            validation_data=val_gen,\n",
    "            epochs=current_config[\"epochs\"],\n",
    "            callbacks=[\n",
    "                TFKerasPruningCallback(trial, \"val_accuracy\"),\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_accuracy',\n",
    "                    patience=5,\n",
    "                    restore_best_weights=True\n",
    "                )\n",
    "            ],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        return max(history.history['val_accuracy'])\n",
    "    \n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e73b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hyperparameters(config, n_trials=50):\n",
    "    cleanup_gpu_memory()\n",
    "    \n",
    "    # Load data\n",
    "    train_df, val_df, le = load_and_preprocess_data()\n",
    "    num_classes = len(le.classes_)\n",
    "    \n",
    "    # Create objective\n",
    "    objective = create_objective(config, train_df, val_df, num_classes)\n",
    "    \n",
    "    # Run optimization\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=5)\n",
    "    )\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    # Save best parameters\n",
    "    best_params = study.best_params\n",
    "    print(\"Best trial:\")\n",
    "    print(f\"  Value (val_accuracy): {study.best_value}\")\n",
    "    print(\"  Params: \")\n",
    "    for key, value in best_params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    \n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a591d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6538/574296162.py:8: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(num_holes_range=[5, 10], hole_height_range=[0.01, 0.02], hole_width_range=[0.01, 0.02], max_holes=3, max_height=1, max_width=1),\n",
      "[I 2025-05-15 02:51:23,631] A new study created in memory with name: no-name-23d0ee51-8fa4-428f-bcb0-7059b8682dc1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label classes: ['bacterial_leaf_blight' 'bacterial_panicle_blight' 'blast' 'brown_spot'\n",
      " 'dead_heart' 'downy_mildew']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-15 02:51:26,889] Trial 0 finished with value: 0.125 and parameters: {'lr': 0.00010446064000635346, 'batch_size': 128, 'dropout_rate': 0.597055350730155, 'conv_filters': 64}. Best is trial 0 with value: 0.125.\n",
      "[I 2025-05-15 02:51:30,743] Trial 1 finished with value: 0.125 and parameters: {'lr': 0.0003256527343062363, 'batch_size': 64, 'dropout_rate': 0.3249569112855539, 'conv_filters': 128}. Best is trial 0 with value: 0.125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer._make_function.<locals>.multi_step_on_iterator at 0x7f708c3e4d60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer._make_function.<locals>.multi_step_on_iterator at 0x7f70ebcc9940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-15 02:51:34,564] Trial 2 finished with value: 0.25 and parameters: {'lr': 1.5182200682270374e-05, 'batch_size': 128, 'dropout_rate': 0.6378327526015831, 'conv_filters': 128}. Best is trial 2 with value: 0.25.\n",
      "[I 2025-05-15 02:51:37,912] Trial 3 finished with value: 0.125 and parameters: {'lr': 0.00011572009729644789, 'batch_size': 64, 'dropout_rate': 0.5182607165409326, 'conv_filters': 96}. Best is trial 2 with value: 0.25.\n",
      "[I 2025-05-15 02:51:40,926] Trial 4 finished with value: 0.125 and parameters: {'lr': 0.00013632687139692799, 'batch_size': 128, 'dropout_rate': 0.5470709174837067, 'conv_filters': 64}. Best is trial 2 with value: 0.25.\n",
      "[I 2025-05-15 02:51:44,267] Trial 5 finished with value: 0.25 and parameters: {'lr': 0.0006171279950365855, 'batch_size': 64, 'dropout_rate': 0.3589953608110795, 'conv_filters': 64}. Best is trial 2 with value: 0.25.\n",
      "[I 2025-05-15 02:51:48,077] Trial 6 finished with value: 0.125 and parameters: {'lr': 3.243206319839892e-05, 'batch_size': 64, 'dropout_rate': 0.5204170361935654, 'conv_filters': 64}. Best is trial 2 with value: 0.25.\n",
      "[I 2025-05-15 02:51:51,895] Trial 7 finished with value: 0.25 and parameters: {'lr': 2.4506252993338724e-05, 'batch_size': 64, 'dropout_rate': 0.6338952584110992, 'conv_filters': 128}. Best is trial 2 with value: 0.25.\n",
      "[I 2025-05-15 02:51:55,712] Trial 8 finished with value: 0.125 and parameters: {'lr': 0.0005268728859907893, 'batch_size': 64, 'dropout_rate': 0.6364603043094875, 'conv_filters': 128}. Best is trial 2 with value: 0.25.\n",
      "[I 2025-05-15 02:51:58,691] Trial 9 finished with value: 0.25 and parameters: {'lr': 4.298475221582097e-05, 'batch_size': 64, 'dropout_rate': 0.42374316963932873, 'conv_filters': 64}. Best is trial 2 with value: 0.25.\n",
      "[I 2025-05-15 02:52:02,517] Trial 10 finished with value: 0.25 and parameters: {'lr': 1.0455267247628347e-05, 'batch_size': 32, 'dropout_rate': 0.6870670983502043, 'conv_filters': 96}. Best is trial 2 with value: 0.25.\n",
      "[I 2025-05-15 02:52:06,782] Trial 11 finished with value: 0.25 and parameters: {'lr': 0.0009932906113957092, 'batch_size': 128, 'dropout_rate': 0.37580750177202993, 'conv_filters': 128}. Best is trial 2 with value: 0.25.\n",
      "[I 2025-05-15 02:52:09,867] Trial 12 finished with value: 0.125 and parameters: {'lr': 1.0030486530781172e-05, 'batch_size': 32, 'dropout_rate': 0.42753814862748374, 'conv_filters': 64}. Best is trial 2 with value: 0.25.\n",
      "[I 2025-05-15 02:52:14,140] Trial 13 finished with value: 0.125 and parameters: {'lr': 0.0002801499614265461, 'batch_size': 128, 'dropout_rate': 0.4533078352408579, 'conv_filters': 128}. Best is trial 2 with value: 0.25.\n",
      "[I 2025-05-15 02:52:17,923] Trial 14 finished with value: 0.25 and parameters: {'lr': 6.045668053658454e-05, 'batch_size': 128, 'dropout_rate': 0.3077111528973008, 'conv_filters': 96}. Best is trial 2 with value: 0.25.\n",
      "[I 2025-05-15 02:52:21,717] Trial 15 finished with value: 0.125 and parameters: {'lr': 0.000935506382016839, 'batch_size': 32, 'dropout_rate': 0.573438677945666, 'conv_filters': 64}. Best is trial 2 with value: 0.25.\n",
      "[I 2025-05-15 02:52:25,516] Trial 16 finished with value: 0.125 and parameters: {'lr': 1.8516637767756328e-05, 'batch_size': 128, 'dropout_rate': 0.6927835821910795, 'conv_filters': 128}. Best is trial 2 with value: 0.25.\n",
      "[I 2025-05-15 02:52:28,519] Trial 17 finished with value: 0.125 and parameters: {'lr': 0.00021761879530120617, 'batch_size': 128, 'dropout_rate': 0.3678777527442144, 'conv_filters': 64}. Best is trial 2 with value: 0.25.\n",
      "[I 2025-05-15 02:52:32,336] Trial 18 finished with value: 0.125 and parameters: {'lr': 6.133255598479519e-05, 'batch_size': 64, 'dropout_rate': 0.47120251118264184, 'conv_filters': 128}. Best is trial 2 with value: 0.25.\n"
     ]
    }
   ],
   "source": [
    "config_1 = {\n",
    "    **config, \n",
    "    \"target_size\": (255, 255),\n",
    "    \"input_shape\": (255, 255, 3),\n",
    "    \"augmentation\": [\n",
    "        A.Resize(width=255, height=255),\n",
    "        A.HueSaturationValue(p=0.5),\n",
    "        A.CoarseDropout(num_holes_range=[5, 10], hole_height_range=[0.01, 0.02], hole_width_range=[0.01, 0.02], max_holes=3, max_height=1, max_width=1),\n",
    "\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Run optimization\n",
    "best_params_1 = optimize_hyperparameters(config_1, n_trials=10)\n",
    "\n",
    "# Update config with best parameters\n",
    "optimized_config_1 = config_1.copy()\n",
    "optimized_config_1.update(best_params_1)\n",
    "\n",
    "# Train final model with optimized parameters\n",
    "final_model_1, final_history = train(optimized_config_1)\n",
    "\n",
    "# Save optimized model\n",
    "final_model_1.save(os.path.join(config[\"save_dir\"], 'optimized_model_1.keras'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
