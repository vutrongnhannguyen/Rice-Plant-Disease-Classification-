{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2dc8e7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "config = {\n",
    "    \"csv_path\": \"processed_data/cleaned_imbalance_metadata.csv\",\n",
    "    \"label_encoder_path\": \"processed_data/le_cleaned_imbalance_metadata.npy\",\n",
    "    \"val_set_csv\": \"processed_data/new_val_metadata.csv\",\n",
    "    \"balanced_train_csv\": \"processed_data/new_balanced_train_metadata.csv\",\n",
    "    \"original_images_dir\": \"Dataset/train_images\",\n",
    "    \"augmented_images_dir\": \"Dataset/SMOT_images_temp\",\n",
    "    \"merged_output_dir\": \"Dataset/train_images_balanced\",\n",
    "\n",
    "}\n",
    "\n",
    "def load_and_preprocess_data(random_state=42):\n",
    "    df = pd.read_csv(config[\"csv_path\"])\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    df['label_encoded'] = le.fit_transform(df['label'])\n",
    "    print(f\"Label classes: {le.classes_}\")\n",
    "    \n",
    "    with open(config['label_encoder_path'], 'wb') as f:\n",
    "        np.save(f, le.classes_)\n",
    "    \n",
    "    train_df, val_df = train_test_split(\n",
    "        df, \n",
    "        test_size=0.2, \n",
    "        stratify=df['label'],\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    \n",
    "    train_df.to_csv(config['train_set_csv'], index=False)\n",
    "    val_df.to_csv(config['val_set_csv'], index=False)\n",
    "    \n",
    "    return train_df, val_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "72f4819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_images_based_on_csv(csv_path, src_dir, dest_dir):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        src = os.path.join(src_dir, row['label'], row['image_id'])\n",
    "        dst = os.path.join(dest_dir, row['label'], row['image_id'])\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        shutil.copy2(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c368c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_augmented_images(df_resampled, original_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    aug = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.CLAHE(p=0.5),\n",
    "        A.HueSaturationValue(p=0.5, hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20),\n",
    "        A.RandomBrightnessContrast(p=0.5, brightness_limit=0.2, contrast_limit=0.2),\n",
    "        A.GaussNoise(p=0.5, var_limit=(10.0, 50.0)),        \n",
    "    ])\n",
    "\n",
    "    # Track existing augmented images to avoid duplicates\n",
    "    existing_augmented = set()\n",
    "    for label in os.listdir(output_dir):\n",
    "        label_dir = os.path.join(output_dir, label)\n",
    "        if os.path.isdir(label_dir):\n",
    "            existing_augmented.update(\n",
    "                os.path.join(label, f) \n",
    "                for f in os.listdir(label_dir) \n",
    "                if f.endswith('.jpg')\n",
    "            )\n",
    "\n",
    "    generated_count = 0\n",
    "\n",
    "    for _, row in tqdm(df_resampled.iterrows(), total=len(df_resampled)):\n",
    "        base_name = os.path.splitext(row['image_id'])[0]\n",
    "        new_filename = f\"{base_name}_aug_{random.randint(1,100000)}.jpg\"\n",
    "        output_path = os.path.join(output_dir, row['label'], new_filename)        \n",
    "\n",
    "            \n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        # Load original image\n",
    "        original_path = os.path.join(original_dir, row['label'], row['image_id'])\n",
    "        original_img = cv2.imread(original_path)\n",
    "        if original_img is None:\n",
    "            continue\n",
    "            \n",
    "        # Generate and save augmented version\n",
    "        augmented = aug(image=original_img)['image']\n",
    "        cv2.imwrite(output_path, augmented)\n",
    "        generated_count += 1\n",
    "\n",
    "    print(f\"Generated {generated_count} synthetic images in {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cfeca7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(original_dir, smote_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Merges original and SMOTE-augmented images into a single dataset.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Iterate through all class subdirectories\n",
    "    for label in tqdm(os.listdir(original_dir)):\n",
    "        original_label_dir = os.path.join(original_dir, label)\n",
    "        smote_label_dir = os.path.join(smote_dir, label)\n",
    "        output_label_dir = os.path.join(output_dir, label)\n",
    "        \n",
    "        os.makedirs(output_label_dir, exist_ok=True)\n",
    "        \n",
    "        for img_file in os.listdir(original_label_dir):\n",
    "            src = os.path.join(original_label_dir, img_file)\n",
    "            dst = os.path.join(output_label_dir, img_file)\n",
    "            if not os.path.exists(dst):\n",
    "                shutil.copy2(src, dst)\n",
    "        \n",
    "        if os.path.exists(smote_label_dir):\n",
    "            for img_file in os.listdir(smote_label_dir):\n",
    "                src = os.path.join(smote_label_dir, img_file)\n",
    "                dst = os.path.join(output_label_dir, img_file)\n",
    "                if not os.path.exists(dst):\n",
    "                    shutil.copy2(src, dst)\n",
    "\n",
    "    print(f\"Merged dataset created at: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "84667270",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_images_in_folders(data_dir):\n",
    "    counts = defaultdict(int)\n",
    "    \n",
    "    for label in os.listdir(data_dir):\n",
    "        label_dir = os.path.join(data_dir, label)\n",
    "        if os.path.isdir(label_dir):\n",
    "            counts[label] = len([\n",
    "                f for f in os.listdir(label_dir) \n",
    "                if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "            ])\n",
    "            \n",
    "    folder_counts = dict(counts)\n",
    "    \n",
    "    print(\"=== Image Counts by Label (From Folders) ===\")\n",
    "    for label, count in folder_counts.items():\n",
    "        print(f\"{label}: {count} images\")\n",
    "    print(f\"TOTAL: {sum(folder_counts.values())} images\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dab07a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_dataset_from_smote_csv(smote_csv_path, original_dir, output_dir, augmentations_per_image=1):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    df = pd.read_csv(smote_csv_path)\n",
    "    \n",
    "    aug = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.CLAHE(p=0.5),\n",
    "        A.HueSaturationValue(p=0.5),\n",
    "        A.GaussianBlur(blur_limit=(3, 7), p=0.3),\n",
    "    ])\n",
    "\n",
    "    for row_idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        class_dir = os.path.join(original_dir, row['label'])\n",
    "        available_images = [f for f in os.listdir(class_dir) if f.endswith('.jpg')]\n",
    "        original_img = cv2.imread(os.path.join(class_dir, random.choice(available_images)))\n",
    "        \n",
    "        # Generate N augmented versions\n",
    "        for aug_idx in range(augmentations_per_image):\n",
    "            output_path = os.path.join(output_dir, row['label'], row['image_id'])\n",
    "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "            \n",
    "            # Apply augmentation and save\n",
    "            augmented = aug(image=original_img)['image']\n",
    "            cv2.imwrite(output_path, augmented)\n",
    "\n",
    "    print(f\"Generated {len(df)*augmentations_per_image} augmented images in {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "89477352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_imbalance_and_augment(config):\n",
    "    \"\"\"Complete pipeline for handling imbalance using SMOTE (fixed ValueError)\"\"\"\n",
    "    \n",
    "    df = pd.read_csv(config[\"csv_path\"])\n",
    "    le = LabelEncoder()\n",
    "    df['label_encoded'] = le.fit_transform(df['label'])\n",
    "    \n",
    "    train_df, val_df = train_test_split(\n",
    "        df,\n",
    "        test_size=0.2,\n",
    "        stratify=df['label'],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Before SMOTE ===\")\n",
    "    print(train_df['label'].value_counts())\n",
    "    \n",
    "    X = train_df.select_dtypes(include=['int64', 'float64']).drop(columns=['label_encoded'])\n",
    "    y = train_df['label_encoded']\n",
    "    \n",
    "    # Apply SMOTE (only on numerical features)\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    \n",
    "    train_df_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    train_df_resampled['label_encoded'] = y_resampled\n",
    "    train_df_resampled['label'] = le.inverse_transform(y_resampled)\n",
    "    \n",
    "    original_ids = train_df['image_id'].tolist()\n",
    "    max_id = max(int(os.path.splitext(img_id)[0]) for img_id in original_ids if img_id.endswith('.jpg'))\n",
    "    \n",
    "    new_ids = [f\"{max_id + 1 + i}.jpg\" for i in range(len(X_resampled) - len(X))]\n",
    "    \n",
    "    # Combine original + new SMOTE IDs\n",
    "    train_df_resampled['image_id'] = original_ids + new_ids\n",
    "    \n",
    "    print(\"\\n=== After SMOTE ===\")\n",
    "    print(train_df_resampled['label'].value_counts())\n",
    "    \n",
    "    train_df_resampled.to_csv(config[\"balanced_train_csv\"], index=False)\n",
    "    val_df.to_csv(config[\"val_set_csv\"], index=False)\n",
    "    \n",
    "    # Generate augmented dataset\n",
    "    generate_final_dataset_from_smote_csv(\n",
    "        config[\"balanced_train_csv\"], \n",
    "        config[\"original_images_dir\"], \n",
    "        config[\"merged_output_dir\"]\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Final Counts ===\")\n",
    "    return count_images_in_folders(config[\"merged_output_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e6951667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Before SMOTE ===\n",
      "label\n",
      "normal                      1393\n",
      "blast                       1364\n",
      "hispa                       1245\n",
      "dead_heart                  1130\n",
      "tungro                       856\n",
      "brown_spot                   749\n",
      "downy_mildew                 487\n",
      "bacterial_leaf_blight        373\n",
      "bacterial_leaf_streak        294\n",
      "bacterial_panicle_blight     269\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== After SMOTE ===\n",
      "label\n",
      "normal                      1393\n",
      "dead_heart                  1393\n",
      "blast                       1393\n",
      "tungro                      1393\n",
      "hispa                       1393\n",
      "bacterial_leaf_blight       1393\n",
      "brown_spot                  1393\n",
      "downy_mildew                1393\n",
      "bacterial_panicle_blight    1393\n",
      "bacterial_leaf_streak       1393\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13930 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13930/13930 [01:01<00:00, 226.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 13930 augmented images in Dataset/train_images_balanced\n",
      "\n",
      "=== Final Counts ===\n",
      "=== Image Counts by Label (From Folders) ===\n",
      "normal: 1393 images\n",
      "dead_heart: 1393 images\n",
      "blast: 1393 images\n",
      "tungro: 1393 images\n",
      "hispa: 1393 images\n",
      "bacterial_leaf_blight: 1393 images\n",
      "brown_spot: 1393 images\n",
      "downy_mildew: 1393 images\n",
      "bacterial_panicle_blight: 1393 images\n",
      "bacterial_leaf_streak: 1393 images\n",
      "TOTAL: 13930 images\n",
      "\n",
      "=== Augmented Images Directory ===\n",
      "None\n",
      "\n",
      "=== Original Images Directory ===\n",
      "=== Image Counts by Label (From Folders) ===\n",
      "bacterial_leaf_blight: 479 images\n",
      "bacterial_leaf_streak: 380 images\n",
      "bacterial_panicle_blight: 337 images\n",
      "blast: 1738 images\n",
      "brown_spot: 965 images\n",
      "dead_heart: 1442 images\n",
      "downy_mildew: 620 images\n",
      "hispa: 1594 images\n",
      "normal: 1764 images\n",
      "tungro: 1088 images\n",
      "TOTAL: 10407 images\n",
      "None\n",
      "\n",
      "=== Merged Output Directory ===\n",
      "=== Image Counts by Label (From Folders) ===\n",
      "normal: 1393 images\n",
      "dead_heart: 1393 images\n",
      "blast: 1393 images\n",
      "tungro: 1393 images\n",
      "hispa: 1393 images\n",
      "bacterial_leaf_blight: 1393 images\n",
      "brown_spot: 1393 images\n",
      "downy_mildew: 1393 images\n",
      "bacterial_panicle_blight: 1393 images\n",
      "bacterial_leaf_streak: 1393 images\n",
      "TOTAL: 13930 images\n",
      "None\n",
      "\n",
      "=== Final Counts ===\n",
      "=== Image Counts by Label (From Folders) ===\n",
      "normal: 1393 images\n",
      "dead_heart: 1393 images\n",
      "blast: 1393 images\n",
      "tungro: 1393 images\n",
      "hispa: 1393 images\n",
      "bacterial_leaf_blight: 1393 images\n",
      "brown_spot: 1393 images\n",
      "downy_mildew: 1393 images\n",
      "bacterial_panicle_blight: 1393 images\n",
      "bacterial_leaf_streak: 1393 images\n",
      "TOTAL: 13930 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "merged_output_dir = handle_imbalance_and_augment(config)\n",
    "print(\"\\n=== Augmented Images Directory ===\")\n",
    "print(merged_output_dir)\n",
    "print(\"\\n=== Original Images Directory ===\")\n",
    "print(count_images_in_folders(config[\"original_images_dir\"]))\n",
    "print(\"\\n=== Merged Output Directory ===\")\n",
    "print(count_images_in_folders(config[\"merged_output_dir\"]))\n",
    "print(\"\\n=== Final Counts ===\")\n",
    "final_counts = count_images_in_folders(config[\"merged_output_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "36f1ca45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_id\n",
      "109473.jpg    1\n",
      "111537.jpg    1\n",
      "111526.jpg    1\n",
      "111527.jpg    1\n",
      "111528.jpg    1\n",
      "             ..\n",
      "102102.jpg    1\n",
      "102308.jpg    1\n",
      "101220.jpg    1\n",
      "104755.jpg    1\n",
      "116176.jpg    1\n",
      "Name: count, Length: 13930, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_resampled = pd.read_csv(config[\"balanced_train_csv\"])\n",
    "print(df_resampled['image_id'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
